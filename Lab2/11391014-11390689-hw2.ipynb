{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval I #\n",
    "## Assignment 2: retrieval models [100 points + 10 bonus points] ##\n",
    "**TA**: Christophe Van Gysel (cvangysel@uva.nl; C3.258B, Science Park 904)\n",
    "\n",
    "**Secondary TAs**: Harrie Oosterhuis, Nikos Voskarides\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will get familiar with basic information retrieval concepts. You will implement and evaluate different information retrieval ranking models and evaluate their performance.\n",
    "\n",
    "We provide you with a VirtualBox image that comes pre-loaded with an index and a Python installation. To query the index, you'll use a Python package ([pyndri](https://github.com/cvangysel/pyndri)) that allows easy access to the underlying document statistics.\n",
    "\n",
    "For evaluation you'll use the [TREC Eval](https://github.com/usnistgov/trec_eval) utility, provided by the National Institute of Standards and Technology of the United States. TREC Eval is the de facto standard way to compute Information Retrieval measures and is frequently referenced in scientific papers.\n",
    "\n",
    "This is a **groups-of-two assignment**, the deadline is **23:59 - 25 January, 2017**. Code quality, informative comments and convincing analysis of the results will be considered when grading. Submission should be done through blackboard, questions can be asked on the course [Piazza](https://piazza.com/class/ixoz63p156g1ts).\n",
    "\n",
    "### Technicalities (must-read!) ###\n",
    "This assignment comes pre-loaded on a VirtualBox running Ubuntu. We have configured the indexing software and Python environment such that it works out of the box. You are allowed to extract the files from the VirtualBox and set-up your own non-virtualized environment. However, in this case you are on your own w.r.t. software support.\n",
    "\n",
    "The assignment directory is organized as follows:\n",
    "   * `./assignment.ipynb` (this file): the description of the assignment.\n",
    "   * `./index/`: the index we prepared for you.\n",
    "   * `./ap_88_90/`: directory with ground-truth and evaluation sets:\n",
    "      * `qrel_test`: test query relevance collection (**test set**).\n",
    "      * `qrel_validation`: validation query relevance collection (**validation set**).\n",
    "      * `topics_title`: semicolon-separated file with query identifiers and terms.\n",
    "      \n",
    "`Python + Jupyter`, `Indri`, `Gensim` and `Pyndri` come pre-installed (see `$HOME/.local`). TREC Eval can be found in `$HOME/Downloads/trec_eval.9.0`. The password of the `student` account on the VirtualBox is `datascience`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREC Eval primer ###\n",
    "The TREC Eval utility can be downloaded and compiled as follows:\n",
    "\n",
    "    git clone https://github.com/usnistgov/trec_eval.git\n",
    "    cd trec_eval\n",
    "    make\n",
    "\n",
    "TREC Eval computes evaluation scores given two files: ground-truth information regarding relevant documents, named *query relevance* or *qrel*, and a ranking of documents for a set of queries, referred to as a *run*. The *qrel* will be supplied by us and should not be changed. For every retrieval model (or combinations thereof) you will generate a run of the top-1000 documents for every query. The format of the *run* file is as follows:\n",
    "\n",
    "    $query_identifier Q0 $document_identifier $rank_of_document_for_query $query_document_similarity $run_identifier\n",
    "    \n",
    "where\n",
    "   * `$query_identifier` is the unique identifier corresponding to a query (usually this follows a sequential numbering).\n",
    "   * `Q0` is a legacy field that you can ignore.\n",
    "   * `$document_identifier` corresponds to the unique identifier of a document (e.g., APXXXXXXX where AP denotes the collection and the Xs correspond to a unique numerical identifier).\n",
    "   * `$rank_of_document_for_query` denotes the rank of the document for the particular query. This field is ignored by TREC Eval and is only maintained for legacy support. The ranks are computed by TREC Eval itself using the `$query_document_similarity` field (see next). However, it remains good practice to correctly compute this field.\n",
    "   * `$query_document_similarity` is a score indicating the similarity between query and document where a higher score denotes greater similarity.\n",
    "   * `$run_identifier` is an identifier of the run. This field is for your own convenience and has no purpose beyond bookkeeping.\n",
    "   \n",
    "For example, say we have two queries: `Q1` and `Q2` and we rank three documents (`DOC1`, `DOC2`, `DOC3`). For query `Q1`, we find the following similarity scores `score(Q1, DOC1) = 1.0`, `score(Q1, DOC2) = 0.5`, `score(Q1, DOC3) = 0.75`; and for `Q2`: `score(Q2, DOC1) = -0.1`, `score(Q2, DOC2) = 1.25`, `score(Q1, DOC3) = 0.0`. We can generate run using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 39,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485532934540,
     "user": {
      "color": "#1FA15D",
      "displayName": "Dana Kianfar",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "04679598140879219033",
      "photoUrl": "//lh4.googleusercontent.com/-wuXN9UJDeVQ/AAAAAAAAAAI/AAAAAAAAL38/QdqQHlnDG58/s50-c-k-no/photo.jpg",
      "sessionId": "4ed2378f242e9e83",
      "userId": "112529955284751587675"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "def write_run(model_name, data, out_f,\n",
    "              max_objects_per_query=sys.maxsize,\n",
    "              skip_sorting=False):\n",
    "    \"\"\"\n",
    "    Write a run to an output file.\n",
    "    Parameters:\n",
    "        - model_name: identifier of run.\n",
    "        - data: dictionary mapping topic_id to object_assesments;\n",
    "            object_assesments is an iterable (list or tuple) of\n",
    "            (relevance, object_id) pairs.\n",
    "            The object_assesments iterable is sorted by decreasing order.\n",
    "        - out_f: output file stream.\n",
    "        - max_objects_per_query: cut-off for number of objects per query.\n",
    "    \"\"\"\n",
    "    for subject_id, object_assesments in data.items():\n",
    "        if not object_assesments:\n",
    "            logging.warning('Received empty ranking for %s; ignoring.',\n",
    "                            subject_id)\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Probe types, to make sure everything goes alright.\n",
    "        # assert isinstance(object_assesments[0][0], float) or \\\n",
    "        #     isinstance(object_assesments[0][0], np.float32)\n",
    "        assert isinstance(object_assesments[0][1], str) or \\\n",
    "            isinstance(object_assesments[0][1], bytes)\n",
    "\n",
    "        if not skip_sorting:\n",
    "            object_assesments = sorted(object_assesments, reverse=True)\n",
    "\n",
    "        if max_objects_per_query < sys.maxsize:\n",
    "            object_assesments = object_assesments[:max_objects_per_query]\n",
    "\n",
    "        if isinstance(subject_id, bytes):\n",
    "            subject_id = subject_id.decode('utf8')\n",
    "\n",
    "        for rank, (relevance, object_id) in enumerate(object_assesments):\n",
    "            if isinstance(object_id, bytes):\n",
    "                object_id = object_id.decode('utf8')\n",
    "\n",
    "            out_f.write(\n",
    "                '{subject} Q0 {object} {rank} {relevance} '\n",
    "                '{model_name}\\n'.format(\n",
    "                    subject=subject_id,\n",
    "                    object=object_id,\n",
    "                    rank=rank + 1,\n",
    "                    relevance=relevance,\n",
    "                    model_name=model_name))\n",
    "            \n",
    "# The following writes the run to standard output.\n",
    "# In your code, you should write the runs to local\n",
    "# storage in order to pass them to trec_eval.\n",
    "with open('./example.run','w') as f:\n",
    "    write_run(\n",
    "        model_name='example',\n",
    "        data={\n",
    "            'Q1': ((1.0, 'DOC1'), (0.5, 'DOC2'), (0.75, 'DOC3')),\n",
    "            'Q2': ((-0.1, 'DOC1'), (1.25, 'DOC2'), (0.0, 'DOC3')),\n",
    "        },\n",
    "        out_f=f,\n",
    "        max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that we know that `DOC1` is relevant and `DOC3` is non-relevant for `Q1`. In addition, for `Q2` we only know of the relevance of `DOC3`. The query relevance file looks like:\n",
    "\n",
    "    Q1 0 DOC1 1\n",
    "    Q1 0 DOC3 0\n",
    "    Q2 0 DOC3 1\n",
    "    \n",
    "We store the run and qrel in files `example.run` and `example.qrel` respectively on disk. We can now use TREC Eval to compute evaluation measures. In this example, we're only interested in Mean Average Precision and we'll only show this below for brevity. However, TREC Eval outputs much more information such as NDCG, recall, precision, etc.\n",
    "\n",
    "    $ trec_eval -m all_trec -q example.qrel example.run | grep -E \"^map\\s\"\n",
    "    > map                   \tQ1\t1.0000\n",
    "    > map                   \tQ2\t0.5000\n",
    "    > map                   \tall\t0.7500\n",
    "    \n",
    "Now that we've discussed the output format of rankings and how you can compute evaluation measures from these rankings, we'll now proceed with an overview of the indexing framework you'll use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyndri primer ###\n",
    "For this assignment you will use [Pyndri](https://github.com/cvangysel/pyndri) [[1](https://arxiv.org/abs/1701.00749)], a python interface for [Indri](https://www.lemurproject.org/indri.php). We have indexed the document collection and you can query the index using Pyndri. We will start by giving you some examples of what Pyndri can do:\n",
    "\n",
    "First we read the document collection index with Pyndri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 2,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485532732725,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "import pyndri\n",
    "\n",
    "index = pyndri.Index('index/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded index can be used to access a collection of documents in an easy manner. We'll give you some examples to get some idea of what it can do, it is up to you to figure out how to use it for the remainder of the assignment.\n",
    "\n",
    "First let's look at the number of documents, since Pyndri indexes the documents using incremental identifiers we can simply take the lowest index and the maximum document and consider the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 3,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485532733659,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "print(\"There are %d documents in this collection.\" % (index.maximum_document() - index.document_base()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the first document out of the collection and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 4,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485532734571,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "example_document = index.document(index.document_base())\n",
    "print(example_document, 'lol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a document consists of two things, a string representing the external document identifier and an integer list representing the identifiers of words that make up the document. Pyndri uses integer representations for words or terms, thus a token_id is an integer that represents a word whereas the token is the actual text of the word/term. Every id has a unique token and vice versa with the exception of stop words: words so common that there are uninformative, all of these receive the zero id.\n",
    "\n",
    "To see what some ids and their matching tokens we take a look at the dictionary of the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 5,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485532736072,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "token2id, id2token, id2df = index.get_dictionary()\n",
    "print(list(id2token.items())[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this dictionary we can see the tokens for the (non-stop) words in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 6,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485532737056,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "print([id2token[word_id] for word_id in example_document[1] if word_id > 0])\n",
    "print(max(id2token.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse can also be done, say we want to look for news about the \"University of Massachusetts\", the tokens of that query can be converted to ids using the reverse dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 7,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485532738357,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "query_tokens = index.tokenize(\"University of Massachusetts\")\n",
    "print(\"Query by tokens:\", query_tokens)\n",
    "query_id_tokens = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "print(\"Query by ids with stopwords:\", query_id_tokens)\n",
    "query_id_tokens = [word_id for word_id in query_id_tokens if word_id > 0]\n",
    "print(\"Query by ids without stopwords:\", query_id_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally we can now match the document and query in the id space, let's see how often a word from the query occurs in our example document:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is certainly not everything Pyndri can do, it should give you an idea of how to use it. Please take a look at the [examples](https://github.com/cvangysel/pyndri) as it will help you a lot with this assignment.\n",
    "\n",
    "**CAUTION**: Avoid printing out the whole index in this Notebook as it will generate a lot of output and is likely to corrupt the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 8,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485532740248,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "matching_words = sum([True for word_id in example_document[1] if word_id in query_id_tokens])\n",
    "print(\"Document %s has %d word matches with query: \\\"%s\\\".\" % (example_document[0], matching_words, ' '.join(query_tokens)))\n",
    "print(\"Document %s and query \\\"%s\\\" have a %.01f%% overlap.\" % (example_document[0], ' '.join(query_tokens),matching_words/float(len(example_document[1]))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the query file\n",
    "You can parse the query file (`ap_88_89/topics_title`) using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 9,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485532741937,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "def parse_topics(file_or_files,\n",
    "                 max_topics=sys.maxsize, delimiter=';'):\n",
    "    assert max_topics >= 0 or max_topics is None\n",
    "\n",
    "    topics = collections.OrderedDict()\n",
    "\n",
    "    if not isinstance(file_or_files, list) and \\\n",
    "            not isinstance(file_or_files, tuple):\n",
    "        if hasattr(file_or_files, '__iter__'):\n",
    "            file_or_files = list(file_or_files)\n",
    "        else:\n",
    "            file_or_files = [file_or_files]\n",
    "\n",
    "    for f in file_or_files:\n",
    "        assert isinstance(f, io.IOBase)\n",
    "\n",
    "        for line in f:\n",
    "            assert(isinstance(line, str))\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            topic_id, terms = line.split(delimiter, 1)\n",
    "\n",
    "            if topic_id in topics and (topics[topic_id] != terms):\n",
    "                    logging.error('Duplicate topic \"%s\" (%s vs. %s).',\n",
    "                                  topic_id,\n",
    "                                  topics[topic_id],\n",
    "                                  terms)\n",
    "\n",
    "            topics[topic_id] = terms\n",
    "\n",
    "            if max_topics > 0 and len(topics) >= max_topics:\n",
    "                break\n",
    "\n",
    "    return topics\n",
    "\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    queries = parse_topics([f_topics])\n",
    "    print('Num of Queries:', len(queries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement and compare lexical IR methods [45 points] ### \n",
    "\n",
    "In this task you will implement a number of lexical methods for IR using the **Pyndri** framework. Then you will evaluate these methods on the dataset we have provided using **TREC Eval**.\n",
    "\n",
    "Use the **Pyndri** framework to get statistics of the documents (term frequency, document frequency, collection frequency; **you are not allowed to use the query functionality of Pyndri**) and implement the following scoring methods in **Python**:\n",
    "\n",
    "- [TF-IDF](http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html). **[5 points]**\n",
    "- [BM25](http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html) with k1=1.2 and b=0.75. **[5 points]**\n",
    "- Language models ([survey](https://drive.google.com/file/d/0B-zklbckv9CHc0c3b245UW90NE0/view))\n",
    "    - Jelinek-Mercer (explore different values of 𝛌 in the range [0.1, 0.2, ..., 0.9]). **[5 points]**\n",
    "    - Dirichlet Prior (explore different values of 𝛍 [500, 1000, ..., 2000]). **[5 points]**\n",
    "    - Absolute discounting (explore different values of 𝛅 in the range [0.1, 0.2, ..., 0.9]). **[5 points]**\n",
    "    - [Positional Language Models](http://sifaka.cs.uiuc.edu/~ylv2/pub/sigir09-plm.pdf) define a language model for each position of a document, and score a document based on the scores of its PLMs. The PLM is estimated based on propagated counts of words within a document through a proximity-based density function, which both captures proximity heuristics and achieves an effect of “soft” passage retrieval. Implement the PLM, all five kernels, but only the Best position strategy to score documents. Use 𝛔 equal to 50, and Dirichlet smoothing with 𝛍 optimized on the validation set (decide how to optimize this value yourself and motivate your decision in the report). **[10 points]**\n",
    "    \n",
    "Implement the above methods and report evaluation measures (on the test set) using the hyper parameter values you optimized on the validation set (also report the values of the hyper parameters). Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "For the language models, create plots showing `NDCG@10` with varying values of the parameters. You can do this by chaining small scripts using shell scripting (preferred) or execute trec_eval using Python's `subprocess`.\n",
    "\n",
    "Compute significance of the results using a [two-tailed paired Student t-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html) **[10 points]**. Be wary of false rejection of the null hypothesis caused by the [multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem). There are multiple ways to mitigate this problem and it is up to you to choose one.\n",
    "\n",
    "Analyse the results by identifying specific queries where different methods succeed or fail and discuss possible reasons that cause these differences.\n",
    "\n",
    "**NOTE**: Don’t forget to use log computations in your calculations to avoid underflows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 11,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485532746028,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "#%load_ext memory_profiler\n",
    "import collections\n",
    "import scipy.spatial\n",
    "import scipy.stats as st\n",
    "import bottleneck\n",
    "from functools import reduce\n",
    "from subprocess import Popen, PIPE\n",
    "import pyndri\n",
    "import pickle\n",
    "import copy\n",
    "import gensim\n",
    "import logging\n",
    "import pyndri.compat\n",
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", color_codes=True, font_scale = 1.3)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 12,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485532746055,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "## Sign/Binomial test\n",
    "# Performs a sign/binomial test on the proportion of impressions where E outperformed P across all ranking pairs.\n",
    "# Inputs:\n",
    "# - Proportions: a vector of statistics representing a difference or ratio of sample A and sample B\n",
    "# - Threshold: a value t which is used to assign the label +1 if the proportions are above t, or -1 if below\n",
    "# The sign test does not take into account examples where both algorithms are tied (a sample value is equal to the threshold)\n",
    "# Returns: p-value\n",
    "def sign_binomial_test(proportions, threshold = 0.5, verbose=True, allow_ties=False, alternative = 'two-sided', sig = 0.05):\n",
    "  k = len(np.where(proportions > threshold)[0])\n",
    "  \n",
    "  if alternative == 'two-sided':\n",
    "    sig /= 2\n",
    "    \n",
    "  if allow_ties:\n",
    "    N = len(proportions)\n",
    "  else:\n",
    "    num_ties = len(np.where(proportions == threshold)[0])\n",
    "    N = len(proportions) - num_ties\n",
    "    \n",
    "  if verbose:\n",
    "    print('Running %s Binomial test on k = %s, n = %s' % (alternative, k, N))\n",
    "    print('Percentage of examples where E>P: %s' % (k * 1.0 / N))\n",
    "\n",
    "  pval = st.binom_test(k, N, 0.5, alternative)\n",
    "\n",
    "  if verbose:\n",
    "    if pval < sig:\n",
    "      print('Should reject H0 given p-val = %s ' % (pval))\n",
    "    else:\n",
    "      print('Not enough evidence to reject H0 given p-val = %s \\n' % (pval))\n",
    "  \n",
    "  return pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 13,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485532772257,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "## Load cache data into memory\n",
    "# To improve performance and reducing IO overhead, at the expensive of high-memory usage, \n",
    "# we load a lot of data into dictionaries for O(1) reading operations\n",
    "\n",
    "# Pyndri\n",
    "index = pyndri.Index('index/')\n",
    "token2id, id2token, id2df = index.get_dictionary() # id2df: id->num of docs containing term\n",
    "id2tf = index.get_term_frequencies() # id->num of occurences in whole collection\n",
    "\n",
    "doc_idx = range(index.document_base(), index.maximum_document()) # range of document indices\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(id2tf) \n",
    "\n",
    "# A dictionary of document_index:bow pairs. Each bow is a dictionary of token_id:tf(tokenid, document). We remove stop words\n",
    "docs_idx = {_d:{k:v for k,v in collections.Counter(index.document(_d)[1]).items() if k != 0} for _d in doc_idx}\n",
    "\n",
    "# A dictionary of document_index:document_length pairs\n",
    "doc_lengths = {_d:float(sum([docs_idx[_d][_t] for _t in docs_idx[_d] if not _t == 0])) for _d in doc_idx}\n",
    "\n",
    "# Average document length\n",
    "avg_doc_length = mean(list(doc_lengths.values()))\n",
    "\n",
    "# Size of collection excluding stop words\n",
    "collection_size_nostop = sum(list(doc_lengths.values()))\n",
    "\n",
    "# number of documents in the collection\n",
    "num_docs = len(docs_idx)\n",
    "\n",
    "del doc_idx\n",
    "\n",
    "# Load validation and test queries into two separate OrderedDicts\n",
    "\n",
    "with open(\"./ap_88_89/qrel_test\", \"r\") as ins:\n",
    "    test_ids = set()\n",
    "    for line in ins:\n",
    "        test_ids.add(line.split(' ')[0])\n",
    "    \n",
    "    test_queries = collections.OrderedDict()\n",
    "    for qid in test_ids:\n",
    "      test_queries[qid] = queries[qid]\n",
    "\n",
    "\n",
    "with open(\"./ap_88_89/qrel_validation\", \"r\") as ins:\n",
    "    val_ids = set()\n",
    "    for line in ins:\n",
    "        val_ids.add(line.split(' ')[0])\n",
    "    \n",
    "    val_queries = collections.OrderedDict()\n",
    "    for qid in val_ids:\n",
    "      val_queries[qid] = queries[qid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 14,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485532772588,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "# Normalizes and tokenizes a string of text\n",
    "# Removes unusual characters, tokenizes and then stems documents\n",
    "# Returns a list of normalized tokens\n",
    "def normalize(text,index):\n",
    "  try:\n",
    "    return [pyndri.stem(x) for x in index.tokenize(pyndri.escape(text)) if not x =='' ]\n",
    "  except(OSError):\n",
    "    print('NORMALIZE ERROR: ', text)\n",
    "  return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 20,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485532953606,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "# Returns the document in position pos in collection index\n",
    "def get_doc(pos, index):\n",
    "  return index.document(pos + index.document_base())\n",
    "\n",
    "# Prints the contents of a document\n",
    "def print_doc(doc):\n",
    "  print([id2token[id] for id in doc[1] if id>0])\n",
    "\n",
    "# Return number of documents in which term t appears\n",
    "def df(t):\n",
    "  return id2df.get(t,0)\n",
    "\n",
    "# Returns idf for term t in a collection of size n documents\n",
    "# Adds 1 to both the numerator and denominator avoid division by zero\n",
    "def idf(t, n):\n",
    "  return np.log(float(n + 1)/(df(t) + 1))\n",
    "\n",
    "# Counts the occurences of term t in document d\n",
    "def tf(t,d):\n",
    "  return np.count_nonzero(np.array(d[1])==t)\n",
    "\n",
    "# Returns the number of times term t occurs in document d (given as a dictionary)\n",
    "def fast_tf(t,d):\n",
    "  return d[t] if t in d else 0\n",
    "\n",
    "# Calculates the tf-idf score of a term t that appears in document d\n",
    "def tfidf(t,d,n,tf_fn = fast_tf):\n",
    "  return np.log(1 + tf_fn(t,d)) * idf(t,n)\n",
    "\n",
    "## Perform sorting on the top k elements of an array\n",
    "# Uses partitioning to create an ordering on left and right sides of the kth element of input array\n",
    "# Then sorts the i=[0,k-1] results in descending order\n",
    "# Inputs:\n",
    "# - values: array to be sorted\n",
    "# - k: cut-off value. Entires before k are sorted\n",
    "# Returns:\n",
    "# - length k array of sorted idx corresponding to input array\n",
    "def top_k_sort(values, k):\n",
    "  \n",
    "  assert k <= len(values), \"Partition index larger than array length\"\n",
    "  \n",
    "  partition_idx = bottleneck.argpartition(values, len(values) - k)[-k:]\n",
    "  partition_score = values[partition_idx]\n",
    "  \n",
    "  sorted_idx = np.argsort(partition_score)[::-1]\n",
    "  \n",
    "  return partition_idx[sorted_idx]\n",
    "\n",
    "# Receives a list of scores for the documents and returns the indices that give the top rank_size ranking \n",
    "# by calling function top_k_sort\n",
    "def rank_results(scores, index, rank_size=1000):\n",
    "  sorted_idx = top_k_sort(scores, rank_size) \n",
    "  return [x+index.document_base() for x in sorted_idx], scores[sorted_idx] # indexing of pyndri docs begin at index.document_base(), so the sorted_idx is adjusted accordingly\n",
    "\n",
    "\n",
    "# Calls TREC EVAL and reports results\n",
    "def run_trec_eval(query_results, model_name, metrics, groundtruth_qrel='./qrel_validation', qrun='./tmp_run', max_results=1000):\n",
    "  \n",
    "  # Create run file\n",
    "  with open(qrun,'w') as f:\n",
    "      write_run(\n",
    "          model_name=model_name,\n",
    "          data=query_results,\n",
    "          out_f=f,\n",
    "          max_objects_per_query=max_results, skip_sorting=False)\n",
    "  \n",
    "  results = np.zeros(len(metrics))\n",
    "  \n",
    "  status_ok = 1\n",
    "  \n",
    "  for m in range(len(metrics)):\n",
    "    p = Popen(['trec_eval -m %s %s %s' % (metrics[m], groundtruth_qrel, qrun)],cwd='/home/student/assignment', stdin=PIPE, stdout=PIPE, stderr=PIPE, shell=True, executable='/bin/bash')\n",
    "    out, err = p.communicate()\n",
    "\n",
    "    if(err):\n",
    "      print('ERROR: ', err.decode())\n",
    "      results[m]= -666\n",
    "      status_ok = 0\n",
    "\n",
    "    results[m] = float(out.decode().split('\\n')[0].split('\\t')[-1])\n",
    "    \n",
    "  return status_ok, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 16,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485532875648,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "# TF-IDF scoring model\n",
    "# Inputs:\n",
    "# - q: query tuple (id, query string)\n",
    "# - index: Pyndri index\n",
    "# - rank_size: number of results to return. Sorting done efficiently here.\n",
    "# - docs_idx: document:bag of words dictionary\n",
    "# Returns: \n",
    "# - sorted_idx: the idx of documents in the index object, sorted by their score\n",
    "# - scores: td-idf scores of each ranking\n",
    "def tfidf_scoring(query, index, docs_idx, doc_lengths, token2id, rank_size = 1000, sort_results = True):\n",
    "  \n",
    "  n = len(docs_idx) # num of docs\n",
    "  \n",
    "  unique_tokens = list(set(normalize(query, index))) # unique tokens in query\n",
    "  token_ids = [token2id[token] for token in unique_tokens if not token == '' and token in token2id] # unique token ids\n",
    "  \n",
    "  scores = np.zeros(len(docs_idx))  # score for each document\n",
    "  i = 0\n",
    "  \n",
    "  for d_idx, doc in docs_idx.items():\n",
    "    \n",
    "    common_tokens = [t_id for t_id in token_ids if t_id in doc] # find common tokens in query and document\n",
    "    \n",
    "    # If document is empty or no lexical matches\n",
    "    if doc_lengths[d_idx] == 0 or len(common_tokens) == 0:\n",
    "      scores[i] = -Inf\n",
    "      i += 1\n",
    "      continue\n",
    "  \n",
    "    for t_id in token_ids:\n",
    "      if t_id in doc:\n",
    "        scores[i] += tfidf(t_id, doc, n) # for each common token, accumulate tfidf score\n",
    "     \n",
    "    scores[i] *= 1\n",
    "    \n",
    "    i+=1\n",
    "  \n",
    "  if not sort_results:\n",
    "    return scores\n",
    "    \n",
    "  return rank_results(scores, index, rank_size) # sorts results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 21,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485533069202,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Runs TFIDF on test queries\n",
    "\n",
    "metrics = ['ndcg_cut.10', 'P.5', 'map_cut.10', 'recall.1000']\n",
    "\n",
    "q_db = test_queries\n",
    "bn_name = 'test'\n",
    "tfidf_test_matrix = np.zeros((len(q_db),len(metrics)))\n",
    "results_dic = {}\n",
    "\n",
    "for qnum, qid in enumerate(q_db):\n",
    "  results_dic.clear()\n",
    "  print('Q ' + str(qnum) + '/' + str(len(q_db)))\n",
    "  idx, scores = tfidf_scoring(q_db[qid], index, docs_idx, doc_lengths, token2id, 1000)\n",
    "  result_ids = [index.document(i)[0] for i in idx]\n",
    "  results_dic[qid] = tuple((scores[i], result_ids[i]) for i in range(len(idx))) \n",
    "  code, res = run_trec_eval(results_dic, 'TFIDF', metrics, groundtruth_qrel='./ap_88_89/qrel_'+bn_name, qrun='./tmp_run', max_results=1000)\n",
    "  tfidf_test_matrix[qnum,:] = np.array(res)\n",
    "   \n",
    "# Store results\n",
    "with open('./tfidf_test_matrix', 'wb') as f:\n",
    "  pickle.dump(tfidf_test_matrix, f)\n",
    "\n",
    "print('Result shape: ' + str(tfidf_test_matrix.shape))\n",
    "print(np.mean(tfidf_test_matrix, axis=0))\n",
    "\n",
    "del tfidf_test_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 23,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485533101487,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "# Performs BM25 scoring on a query string\n",
    "# Default paramters are as instructured in task 1\n",
    "# Returns sorted document indexes and scores\n",
    "def bm25(query, index, docs_idx, doc_lengths, token2id, avg_doc_length, k1=1.2, b=0.75, rank_size=1000, sort_results = True):\n",
    "  \n",
    "    n = len(docs_idx) # num of docs\n",
    "\n",
    "    unique_tokens = list(set(normalize(query, index)))\n",
    "    token_ids = [token2id[token] for token in unique_tokens if not token == '' and token in token2id]\n",
    "\n",
    "    scores = np.zeros(len(docs_idx)) \n",
    "    i = 0\n",
    "    for d_idx, doc in docs_idx.items():\n",
    "\n",
    "        if doc_lengths[d_idx] == 0:\n",
    "            scores[i] = -Inf\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        for t_id in token_ids:\n",
    "            if t_id in doc:  # if term not in document, tf(t,d) = 0, then bm25 score of that term is 0\n",
    "                tf = doc[t_id]\n",
    "                scores[i] += ((k1+1) * tf) / (k1*((1-b) + b*(doc_lengths[d_idx]/avg_doc_length)) + tf) * idf(t_id, n)\n",
    "        i+=1\n",
    "\n",
    "    if not sort_results:\n",
    "        return scores\n",
    "  \n",
    "    return rank_results(scores, index, rank_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 25,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485533220149,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "## BM25 on test queries\n",
    "\n",
    "metrics = ['ndcg_cut.10', 'P.5', 'map_cut.10', 'recall.1000']\n",
    "\n",
    "q_db = test_queries\n",
    "bn_name = 'test'\n",
    "\n",
    "bm25_test_matrix = np.zeros((len(q_db),len(metrics)))\n",
    "\n",
    "\n",
    "results_dic = {}\n",
    "\n",
    "for qnum, qid in enumerate(q_db):  # for each query\n",
    "    \n",
    "    results_dic.clear()\n",
    "    print('Q ' + str(qnum) + '/' + str(len(q_db)))\n",
    "    idx, scores = bm25(q_db[qid], index, docs_idx, doc_lengths, token2id, avg_doc_length, k1=1.2, b=0.75, rank_size=1000) \n",
    "    result_ids = [index.document(i)[0] for i in idx]\n",
    "    results_dic[qid] = tuple((scores[i], result_ids[i]) for i in range(len(idx))) \n",
    "    code, res = run_trec_eval(results_dic, 'BM25', metrics, groundtruth_qrel='./ap_88_89/qrel_'+bn_name, qrun='./tmp_run', max_results=1000)\n",
    "    bm25_test_matrix[qnum,:] = np.array(res)\n",
    "   \n",
    "\n",
    "with open('./bm25_test_matrix', 'wb') as f:\n",
    "    pickle.dump(bm25_test_matrix, f)\n",
    "\n",
    "print('Result shape: ' + str(bm25_test_matrix.shape))\n",
    "print(np.mean(bm25_test_matrix, axis=0))\n",
    "\n",
    "del bm25_test_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jelinek-Mercer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 29,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485533581284,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "# Jelinek-Mercer with query likelihood\n",
    "# Inputs:\n",
    "# - query: query string\n",
    "# - Parameter _lambda \n",
    "# Outputs:\n",
    "# - sorted document index\n",
    "# - sorted corresponding scores\n",
    "def jelinek_mercer(query, index, docs_idx, doc_lengths, collection_size_nostop, term_counts, _lambda, rank_size=1000, sort_results = True):\n",
    "  \n",
    "  n = len(docs_idx) # num of docs\n",
    "  \n",
    "  tokens = normalize(query, index)\n",
    "  token_ids = [token2id[token] for token in tokens if not token == '' and token in token2id]\n",
    "  \n",
    "  scores = np.zeros(len(docs_idx))  # score for each doc\n",
    "  i = 0\n",
    "  for d_idx, doc in docs_idx.items():\n",
    "    \n",
    "    if doc_lengths[d_idx] == 0: # ignore empty docs\n",
    "      scores[i] = -Inf\n",
    "      i += 1\n",
    "      continue\n",
    "    \n",
    "    for t_id in token_ids: # for each token\n",
    "        tf = doc[t_id] if t_id in doc else 0 # get tf\n",
    "        scores[i] += np.log(_lambda * tf / doc_lengths[d_idx] + (1 - _lambda) * term_counts[t_id] / collection_size_nostop)  \n",
    "    i+=1\n",
    "  \n",
    "  if not sort_results:\n",
    "    return scores\n",
    "  \n",
    "  return rank_results(scores, index, rank_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 26,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485453217652,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "f397ca86540612b",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "## JM - optimization on validation queries\n",
    "\n",
    "eval_metric = ['ndcg_cut.10']\n",
    "_lambdas = [0.1,0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "_lambda_scores = np.zeros(len(_lambdas))\n",
    "\n",
    "\n",
    "for i_lambda, _lambda in enumerate(_lambdas):\n",
    "  print('Evaluating lambda=',_lambda)\n",
    "  tmp_res = {}\n",
    "  \n",
    "  for qid, query in val_queries.items(): # run validation set queries\n",
    "    idx, scores = jelinek_mercer(query, index, docs_idx, doc_lengths, collection_size_nostop, id2tf, _lambda, rank_size=1000) \n",
    "    result_ids = [index.document(i)[0] for i in idx]\n",
    "    tmp_res[qid] = tuple((scores[i], result_ids[i]) for i in range(len(idx)))\n",
    "  \n",
    "  print('Running treceval lambda=',_lambda)\n",
    "  code, _lambda_scores[i_lambda] = run_trec_eval(tmp_res, 'JM', eval_metric, groundtruth_qrel='./ap_88_89/qrel_validation', qrun='./tmp_run', max_results=1000)\n",
    "    \n",
    "best_lambda = _lambdas[np.argmax(_lambda_scores)]\n",
    "print('Best lambda:', best_lambda )\n",
    "res = [(_lambdas[i],_lambda_scores[i]) for i in range(len(_lambdas))]\n",
    "print(res)\n",
    "  \n",
    "with open('./jm_val_ndcg', 'wb') as f:\n",
    "  pickle.dump(res, f)\n",
    "  \n",
    "plot(_lambdas, _lambda_scores)\n",
    "plt.ylabel('NDCG@10')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.title('Performance of Jelinek-Mercer on NDCG@10 for several $\\lambda$ values')\n",
    "plt.savefig('./jm_optim.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 31,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485533838793,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "## JM on test queries\n",
    "\n",
    "metrics = ['ndcg_cut.10', 'P.5', 'map_cut.10', 'recall.1000']\n",
    "best_lambda = 0.4\n",
    "q_db = test_queries\n",
    "bn_name = 'test'\n",
    "\n",
    "jm_test_matrix = np.zeros((len(q_db),len(metrics)))\n",
    "\n",
    "results_dic = {}\n",
    "for qnum, qid in enumerate(q_db):\n",
    "  results_dic.clear()\n",
    "  print('Q ' + str(qnum) + '/' + str(len(q_db)))\n",
    "  idx, scores = jelinek_mercer(q_db[qid], index, docs_idx, doc_lengths, collection_size_nostop, id2tf, best_lambda, rank_size=1000) \n",
    "  result_ids = [index.document(i)[0] for i in idx]\n",
    "  results_dic[qid] = tuple((scores[i], result_ids[i]) for i in range(len(idx))) \n",
    "  code, res = run_trec_eval(results_dic, 'JM', metrics, groundtruth_qrel='./ap_88_89/qrel_'+bn_name, qrun='./tmp_run', max_results=1000)\n",
    "  jm_test_matrix[qnum,:] = np.array(res)\n",
    "   \n",
    "with open('./jm_test_matrix', 'wb') as f:\n",
    "  pickle.dump(jm_test_matrix, f)\n",
    "\n",
    "print('Result shape: ' + str(jm_test_matrix.shape))\n",
    "print(np.mean(jm_test_matrix, axis=0))\n",
    "\n",
    "del jm_test_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute Discounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 32,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485534120329,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "# Absolute Discounting with query likelihood \n",
    "# Inputs:\n",
    "# - query: query string\n",
    "# - Parameter delta \n",
    "# Outputs:\n",
    "# - sorted document index\n",
    "# - sorted corresponding scores\n",
    "def abs_discounting(query, index, docs_idx, doc_lengths, collection_size_nostop, term_counts, delta, rank_size=1000, sort_results = True):\n",
    "  \n",
    "  n = len(docs_idx) # num of docs\n",
    "  \n",
    "  tokens = normalize(query, index)\n",
    "  token_ids = [token2id[token] for token in tokens if not token == '' and token in token2id]\n",
    "  \n",
    "  scores = np.zeros(len(docs_idx)) \n",
    "  i = 0\n",
    "  for d_idx, doc in docs_idx.items():\n",
    "    \n",
    "    if doc_lengths[d_idx] == 0:\n",
    "      scores[i] = -Inf\n",
    "      i += 1\n",
    "      continue\n",
    "      \n",
    "    for t_id in token_ids:\n",
    "        tf = doc[t_id] if t_id in doc else 0\n",
    "        scores[i] += np.log( (max( tf - delta, 0)  + delta * len(doc) * (term_counts[t_id] / collection_size_nostop) ) / doc_lengths[d_idx] )  \n",
    "    i+=1\n",
    "  \n",
    "  if not sort_results:\n",
    "    return scores\n",
    "  \n",
    "  return rank_results(scores, index, rank_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 28,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485453731372,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "f397ca86540612b",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "## Abs Discounting - optimization on validation queries\n",
    "# Plots results\n",
    "eval_metric = ['ndcg_cut.10']\n",
    "validation_jm = {}\n",
    "deltas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "delta_scores = np.zeros(len(deltas))\n",
    "\n",
    "\n",
    "for i_delta, delta in enumerate(deltas):\n",
    "  print('Evaluating delta=',delta)\n",
    "  tmp_res = {}\n",
    "  \n",
    "  for qid, query in val_queries.items(): # run validation set queries\n",
    "    idx, scores = abs_discounting(query, index, docs_idx, doc_lengths, collection_size_nostop, id2tf, delta, rank_size=1000)\n",
    "    result_ids = [index.document(i)[0] for i in idx]\n",
    "    tmp_res[qid] = tuple((scores[i], result_ids[i]) for i in range(len(idx)))\n",
    "  \n",
    "  print('Running treceval delta=',delta)\n",
    "  code, delta_scores[i_delta] = run_trec_eval(tmp_res, 'AD', eval_metric, groundtruth_qrel='./ap_88_89/qrel_validation', qrun='./tmp_run', max_results=1000)\n",
    "    \n",
    "best_delta = deltas[np.argmax(delta_scores)]\n",
    "print('Best delta:', best_delta )\n",
    "res = [(deltas[i],delta_scores[i]) for i in range(len(deltas))]\n",
    "print(res)\n",
    "  \n",
    "with open('./ad_val_ndcg', 'wb') as f:\n",
    "  pickle.dump(res, f)\n",
    "  \n",
    "plot(deltas, delta_scores)\n",
    "plt.ylabel('NDCG@10')\n",
    "plt.xlabel('$\\delta$')\n",
    "plt.title('Performance of Abs. Discounting on NDCG@10 for several $\\delta$ values')\n",
    "plt.savefig('./ad_optim.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 33,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485534120395,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "## Abs Discounting on test queries\n",
    "\n",
    "metrics = ['ndcg_cut.10', 'P.5', 'map_cut.10', 'recall.1000']\n",
    "q_db = test_queries\n",
    "bn_name = 'test'\n",
    "best_delta = 0.8\n",
    "\n",
    "ad_test_matrix = np.zeros((len(q_db),len(metrics)))\n",
    "\n",
    "results_dic = {}\n",
    "for qnum, qid in enumerate(q_db):\n",
    "  results_dic.clear()\n",
    "  print('Q ' + str(qnum) + '/' + str(len(q_db)))\n",
    "  idx, scores = abs_discounting(q_db[qid], index, docs_idx, doc_lengths, collection_size_nostop, id2tf, best_delta, rank_size=1000)\n",
    "  result_ids = [index.document(i)[0] for i in idx]\n",
    "  results_dic[qid] = tuple((scores[i], result_ids[i]) for i in range(len(idx))) \n",
    "  code, res = run_trec_eval(results_dic, 'AD', metrics, groundtruth_qrel='./ap_88_89/qrel_'+bn_name, qrun='./tmp_run', max_results=1000)\n",
    "  ad_test_matrix[qnum,:] = np.array(res)\n",
    "   \n",
    "with open('./ad_test_matrix', 'wb') as f:\n",
    "  pickle.dump(ad_test_matrix, f)\n",
    "\n",
    "print('Result shape: ' + str(ad_test_matrix.shape))\n",
    "print(np.mean(ad_test_matrix, axis=0))\n",
    "\n",
    "del ad_test_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dirichlet Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 34,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485534385977,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "# Dirichlet smoothing with query likelihood\n",
    "# Inputs:\n",
    "# - query: query string\n",
    "# - Parameter _lambda \n",
    "# Outputs:\n",
    "# - sorted document index\n",
    "# - sorted corresponding scores\n",
    "def dirichlet_smoothing(query, index, docs_idx, doc_lengths, collection_size_nostop, term_counts, mu, rank_size=1000, sort_results = True):\n",
    "  \n",
    "  tokens = normalize(query, index)\n",
    "  token_ids = [token2id[token] for token in tokens if not token == '' and token in token2id]\n",
    "  \n",
    "  scores = np.zeros(len(docs_idx))\n",
    "  i=0\n",
    "  \n",
    "  for d_idx, doc in docs_idx.items():\n",
    "    \n",
    "    if doc_lengths[d_idx] == 0:\n",
    "      scores[i] = -Inf\n",
    "      i += 1\n",
    "      continue\n",
    "    \n",
    "    for t_id in token_ids:\n",
    "      tf = doc[t_id] if t_id in doc else 0\n",
    "      p = (tf + mu * (term_counts[t_id]/collection_size_nostop)) / (doc_lengths[d_idx] + mu) \n",
    "      scores[i] += np.log(p)\n",
    "      \n",
    "    i+=1\n",
    "  \n",
    "  if not sort_results:\n",
    "    return scores\n",
    "  \n",
    "  return rank_results(scores, index, rank_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 30,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485454019853,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "f397ca86540612b",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "## Dirichlet smoothing - optimization on validation set\n",
    "# Plots results\n",
    "eval_metric = ['ndcg_cut.10']\n",
    "validation_dirichlet = {}\n",
    "mus = [500,1000,1500,2000,3500,5000]\n",
    "mu_scores = np.zeros(len(mus))\n",
    "\n",
    "for i_mu, mu in enumerate(mus):\n",
    "  print('Evaluating mu=',mu)\n",
    "  tmp_res = {}\n",
    "  \n",
    "  for qid, query in val_queries.items(): # run validation set queries\n",
    "    idx, scores = dirichlet_smoothing(query, index, docs_idx, doc_lengths, collection_size_nostop, id2tf, mu, 1000)\n",
    "    result_ids = [index.document(i)[0] for i in idx]\n",
    "    tmp_res[qid] = tuple((scores[i], result_ids[i]) for i in range(len(idx)))\n",
    "  \n",
    "  print('Running treceval mu=',mu)\n",
    "  code, mu_scores[i_mu] = run_trec_eval(tmp_res, 'Dirichlet', eval_metric, groundtruth_qrel='./ap_88_89/qrel_validation', qrun='./tmp_run', max_results=1000)\n",
    "  \n",
    "best_mu = mus[np.argmax(mu_scores)]\n",
    "print('Best mu:', best_mu )\n",
    "res = [(mus[i],mu_scores[i]) for i in range(len(mus))]\n",
    "print(res)\n",
    "  \n",
    "with open('./dir_val_ndcg', 'wb') as f:\n",
    "  pickle.dump(res, f)\n",
    "\n",
    "plot(mus, mu_scores)\n",
    "plt.ylabel('NDCG@10')\n",
    "plt.xlabel('$\\mu$')\n",
    "plt.title('Performance of Dirichlet Smoothing on NDCG@10 for several $\\mu$ values')\n",
    "plt.savefig('./dir_optim.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 35,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485534386164,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "## Dirichlet Discounting on test queries\n",
    "\n",
    "metrics = ['ndcg_cut.10', 'P.5', 'map_cut.10', 'recall.1000']\n",
    "q_db = test_queries\n",
    "bn_name = 'test'\n",
    "best_mu = 2000\n",
    "\n",
    "dir_test_matrix = np.zeros((len(q_db),len(metrics)))\n",
    "\n",
    "results_dic = {}\n",
    "for qnum, qid in enumerate(q_db):\n",
    "  results_dic.clear()\n",
    "  print('Q ' + str(qnum) + '/' + str(len(q_db)))\n",
    "  idx, scores = dirichlet_smoothing(test_queries[qid], index, docs_idx, doc_lengths, collection_size_nostop, id2tf, best_mu, 1000)\n",
    "  result_ids = [index.document(i)[0] for i in idx]\n",
    "  results_dic[qid] = tuple((scores[i], result_ids[i]) for i in range(len(idx))) \n",
    "  code, res = run_trec_eval(results_dic, 'DIR', metrics, groundtruth_qrel='./ap_88_89/qrel_'+bn_name, qrun='./tmp_run', max_results=1000)\n",
    "  dir_test_matrix[qnum,:] = np.array(res)\n",
    "   \n",
    "with open('./dir_test_matrix', 'wb') as f:\n",
    "  pickle.dump(dir_test_matrix, f)\n",
    "\n",
    "print('Result shape: ' + str(dir_test_matrix.shape))\n",
    "print(np.mean(dir_test_matrix, axis=0))\n",
    "\n",
    "del dir_test_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Positional Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 36,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485534386467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "# Heavily based on PLM's paper author Yuanhua Lv's code\n",
    "# http://sifaka.cs.uiuc.edu/~ylv2/pub/plm/PLMRetEval.cpp\n",
    "\n",
    "\n",
    "def passage_kernel(dis, sigma):\n",
    "  return float(np.abs(dis) <= sigma)\n",
    "\n",
    "\n",
    "def gaussian_kernel(dis, sigma):\n",
    "  return np.exp(-dis**2 / (2 * sigma**2))\n",
    "\n",
    "\n",
    "def triangle_kernel(dis, sigma):\n",
    "  if np.abs(dis) <= sigma:\n",
    "    return 1 - np.abs(dis)/sigma\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "def cosine_kernel(dis, sigma):\n",
    "  if np.abs(dis) <= sigma:\n",
    "    return 0.5 * (1 + np.cos(pi*np.abs(dis)/sigma))\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "  \n",
    "def circle_kernel(dis, sigma):\n",
    "  if np.abs(dis) <= sigma:\n",
    "    return np.sqrt(1 - (dis/sigma)**2)\n",
    "  else:\n",
    "    return 0\n",
    "  \n",
    "def gaussian_cdf(x, mean, sigma):\n",
    "  x = (x - mean) / sigma\n",
    "  if x==0:\n",
    "    res = 0.5\n",
    "  else:\n",
    "    t = 1 / (1 + 0.2316419 * np.abs(x))\n",
    "    t *= t * (1 / np.sqrt(2 * pi)) * np.exp(-0.5 * x * x) * (0.31938153 + t * (-0.356563782 + t * (1.781477937 + t * (-1.821255978 + t * 1.330274429))))\n",
    "    if x >= 0:\n",
    "      res = 1 - t\n",
    "    else:\n",
    "      res = t\n",
    "  return sqrt(2 * pi) * sigma * res\n",
    "\n",
    "def triangle_cdf(x, mean, sigma):\n",
    "  x = (x - mean) / sigma\n",
    "  if (x >= 1):\n",
    "    res = sigma\n",
    "  elif(x < -1):\n",
    "    res = 0\n",
    "  elif(x < 0):\n",
    "    res = sigma * (1 - np.abs(x)) * (1 - np.abs(x)) / 2.0\n",
    "  else:\n",
    "    res = sigma - sigma * (1 - x) * (1 - x) / 2.0\n",
    "  return res\n",
    "\n",
    "def cosine_cdf(x, mean, sigma):\n",
    "  x = (x - mean) / sigma\n",
    "  if (x >= 1):\n",
    "    res = sigma\n",
    "  elif(x < -1):\n",
    "    res = 0\n",
    "  elif(x < 0):\n",
    "    res = sigma * (1 + x - np.sin(pi * x) / pi) / 2.0\n",
    "  else:\n",
    "    res = sigma - sigma * (1 - x + np.sin(pi * x) / pi) / 2.0\n",
    "  return res\n",
    "\n",
    "def circle_cdf(x, mean, sigma):\n",
    "  x = (x - mean) / sigma\n",
    "  if (x >= 1):\n",
    "    res = (pi - 2.0) * sigma\n",
    "  elif(x < -1):\n",
    "    res = 0\n",
    "  elif (x < 0):\n",
    "    res = sigma * (np.arcsin(x) + pi / 2.0 - np.sqrt(1 - x * x))\n",
    "  else:\n",
    "    res = (pi - 2.0) * sigma - sigma * (np.arcsin(-x) + pi / 2.0 - np.sqrt(1 - x * x))\n",
    "  return res\n",
    "\n",
    "def passage_cdf(x, mean, sigma):\n",
    "  res = 0\n",
    "  if sigma > x- mean:\n",
    "    res += x - mean\n",
    "  else:\n",
    "    res += sigma\n",
    "\n",
    "  if sigma > mean:\n",
    "    res += mean\n",
    "  else:\n",
    "    res += sigma\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 37,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485534447941,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "# Positional language model with KL-divergence scoring\n",
    "# See report for details into this algorithm\n",
    "# Due to performance considerations, we only re-rank other documents (from tfidf)\n",
    "# Inputs:\n",
    "# - query: query string\n",
    "# - filtered_idx: tfidf top K document indices\n",
    "# - Parameters: mu, kernel, kernel_cdf, sigma\n",
    "# Returns:\n",
    "# - Ranked list of document indices\n",
    "# - Ranked list of corresponding scores\n",
    "def plm(query, index, docs_idx, filtered_idx, doc_lengths, collection_size_nostop, term_counts, mu, kernel, kernel_cumul, sigma, rank_size=1000, sort_results = True):\n",
    "  \n",
    "  tokens = normalize(query, index)# query tokens\n",
    "  token_ids = [token2id[token] for token in tokens if not token == '' and token in token2id]\n",
    "  num_unique_tokens= len(set(token_ids)) \n",
    "  scores = np.zeros(len(filtered_idx)) # score for each doc\n",
    "  \n",
    "  # For each doc\n",
    "  for i, d_idx in enumerate(filtered_idx):\n",
    "    \n",
    "    doc = docs_idx[d_idx]\n",
    "    \n",
    "    common_tokens = [t_id for t_id in token_ids if t_id in doc]\n",
    "    \n",
    "    q_len = len(common_tokens) # number of non-unique common tokens\n",
    "   \n",
    "    if doc_lengths[d_idx] == 0 or q_len == 0:\n",
    "      scores[i] = -Inf\n",
    "      continue\n",
    "    \n",
    "    unique_common_tokens = dict(collections.Counter(common_tokens))\n",
    "    \n",
    "    # Normalized counts of query words in document\n",
    "    #q = np.array([unique_common_tokens[t] for t in unique_common_tokens]) # See proof for simplification on notepad\n",
    "    \n",
    "    # Query vector: uniform prior (division done later)\n",
    "    q = np.ones(len(unique_common_tokens))\n",
    "      \n",
    "    # construct inverted index of positions of common terms in a document\n",
    "    seq = index.document(d_idx)[1]\n",
    "    inv_idx = {t_id:set() for t_id in unique_common_tokens}\n",
    "    for pos,x in enumerate(seq):\n",
    "      if x in inv_idx:\n",
    "        inv_idx[x].add(pos+1)      \n",
    "    \n",
    "    # All positions where commons words occur. As an approximation, we only consider these positions\n",
    "    qwpos = set()\n",
    "    for key in inv_idx:\n",
    "      qwpos = qwpos.union(inv_idx[key])\n",
    "    \n",
    "    p_mu = np.zeros(len(unique_common_tokens)) # probability vector for each word in query\n",
    "    \n",
    "    best_k = -1\n",
    "    best_score = -Inf\n",
    "    \n",
    "    # k is the current position in the document\n",
    "    \n",
    "    for k in qwpos: # positions where query words occur\n",
    "        \n",
    "      Z_k = kernel_cumul(doc_lengths[d_idx],k,sigma) - kernel_cumul(0, k, sigma) # normalizing term\n",
    "                   \n",
    "      for _pos, tok in enumerate(unique_common_tokens):\n",
    "        \n",
    "        sum_kernels = 0\n",
    "        for j in inv_idx[tok]:\n",
    "          sum_kernels += kernel(k - j, sigma) \n",
    "        \n",
    "        p_mu[_pos] =  (sum_kernels + mu * (term_counts[tok]/collection_size_nostop)) / (Z_k + mu)\n",
    "      \n",
    "      # Larger dot product is preferred\n",
    "      kl = np.dot(q, np.log2(p_mu)) \n",
    "      \n",
    "      if kl > best_score:\n",
    "        best_k = k\n",
    "        best_score = kl\n",
    "        \n",
    "    #print('Best Score: ' + str(best_score))\n",
    "    scores[i] = best_score/ len(unique_common_tokens) + np.log2(len(token_ids))  \n",
    "  \n",
    "  if not sort_results:\n",
    "    return scores\n",
    "  \n",
    "  sorted_idx, sorted_scores = rank_results(scores, index, rank_size)\n",
    "  return sorted_idx, sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 126,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485465312494,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "2e35cb58fb89cd77",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "## PLM on validation set\n",
    "import time\n",
    "results_plm = {}\n",
    "mu = 2000 # best value from dirichlet smoothing\n",
    "sigma = 50\n",
    "rank_size=100 # for performance considerations\n",
    "metrics = ['ndcg_cut.10', 'P.5', 'map_cut.10', 'recall.100']\n",
    "\n",
    "kernels = [('circle', circle_kernel, circle_cdf), ('triangle', triangle_kernel, triangle_cdf), \n",
    "           ('cosine', cosine_kernel, cosine_cdf), ('passage',passage_kernel, passage_cdf), ('gaussian',gaussian_kernel, gaussian_cdf)]\n",
    "\n",
    "tmp = {}\n",
    "query_set = val_queries #val_queries\n",
    "\n",
    "# Initialize a dictionary to store results \n",
    "for kernel_name, _, _ in kernels:\n",
    "  results_plm[kernel_name] = np.zeros((len(query_set), len(metrics)))\n",
    "\n",
    "# For each query\n",
    "for i, qid in enumerate(query_set):\n",
    "  \n",
    "  # Rerank BM25 scores\n",
    "  tfidf_idx, tfidf_scores = bm25(query_set[qid], index, docs_idx, doc_lengths, token2id, avg_doc_length, k1=1.2, b=0.75, rank_size=rank_size)\n",
    "  #tfidf_idx, tfidf_scores = tfidf_scoring(query_set[qid], index, docs_idx, doc_lengths, token2id, rank_size)\n",
    "  \n",
    "    # for each kernel\n",
    "  for kernel_name, kernel, kernel_cdf in kernels:\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    idx, scores = plm(query_set[qid], index, docs_idx, tfidf_idx, doc_lengths, collection_size_nostop, id2tf, mu, kernel, kernel_cdf, sigma, rank_size)\n",
    "    \n",
    "    idx = [tfidf_idx[dum-1] for dum in idx] # index correction. ignore this!\n",
    "    \n",
    "    # trec eval\n",
    "    result_ids = [index.document(dum)[0] for dum in idx]\n",
    "    tmp[qid] = tuple((scores[dum], result_ids[dum]) for dum in range(len(idx)))\n",
    "    code, res = run_trec_eval(tmp, 'PLM-'+kernel_name, metrics, groundtruth_qrel='./ap_88_89/qrel_validation', qrun='./tmp_run', max_results=rank_size)\n",
    "    \n",
    "    if code == 0:\n",
    "      print('CODE ZERO',res)\n",
    "      \n",
    "    results_plm[kernel_name][i,:] = np.array(res)\n",
    "    \n",
    "    tmp.clear()\n",
    "    \n",
    "    if i % 5 == 0:\n",
    "      print('- Query %d of %d: Kernel %s, elapsed time: %f' % (i+1,len(query_set), kernel_name, time.time() - start_time))\n",
    "  #break\n",
    "  \n",
    "with open('./plm_perkernel_val_results', 'wb') as f:\n",
    "  pickle.dump(f, results_plm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 127,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485465355967,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "2e35cb58fb89cd77",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "# Plots kernel metrics\n",
    "\n",
    "kn_list = []\n",
    "ndcg_list = []\n",
    "\n",
    "plm_res = np.zeros((len(query_set),len(kernels)))\n",
    "\n",
    "print('')\n",
    "\n",
    "i=0\n",
    "for kname, _, _ in kernels:\n",
    "  kn_list.append(kname)\n",
    "  plm_res[:,i] = results_plm[kname][:,0]\n",
    "  ndcg_list.append(np.mean(results_plm[kname], axis=0)[0])\n",
    "  i+=1\n",
    "\n",
    "plm_val = pd.DataFrame(plm_res, columns=kn_list)\n",
    "\n",
    "sns.barplot(data = plm_val)\n",
    "plt.ylabel('NDCG@10')\n",
    "#plt.title('Performance of each kernel on NDCG@10 with 95% confidence intervals')\n",
    "plt.savefig('./kernel_selection_validation_ndcg.png')\n",
    "\n",
    "_ = sign_binomial_test(plm_res[:,2] - plm_res[:,1], 0, True, alternative='greater');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "ename": "KeyboardInterrupt",
      "engine_info": {
       "engine_id": -1,
       "engine_uuid": "1535238e-0361-49ba-8354-a46754acbcc5",
       "method": "execute"
      },
      "evalue": "",
      "execution_count": 38,
      "payload": [],
      "status": "error",
      "traceback": [
       "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
       "\u001b[1;32m<ipython-input-38-9c34f457c26b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m   \u001b[1;31m# Rerank TFIDF scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m   \u001b[0mtfidf_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mqid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocs_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken2id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mkernel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_cdf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkernels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m<ipython-input-16-3de74cbd83f4>\u001b[0m in \u001b[0;36mtfidf_scoring\u001b[1;34m(query, index, docs_idx, doc_lengths, token2id, rank_size, sort_results)\u001b[0m\n\u001b[0;32m     17\u001b[0m   \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m   \u001b[1;32mfor\u001b[0m \u001b[0md_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs_idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mcommon_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt_id\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtoken_ids\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
      ],
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485534447995,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "## PLM with Triangle Kernel on test set. \n",
    "import time\n",
    "\n",
    "mu = 2000\n",
    "sigma = 50\n",
    "rank_size=1000\n",
    "metrics = ['ndcg_cut.10', 'P.5', 'map_cut.10', 'recall.1000']\n",
    "\n",
    "kernels = [('triangle', triangle_kernel, triangle_cdf)]\n",
    "tmp = {}\n",
    "\n",
    "query_set = test_queries\n",
    "results_plm = np.zeros((len(query_set), len(metrics)))\n",
    "\n",
    "for i, qid in enumerate(query_set):\n",
    "  \n",
    "  # Rerank TFIDF scores\n",
    "  tfidf_idx, tfidf_scores = tfidf_scoring(query_set[qid], index, docs_idx, doc_lengths, token2id, rank_size = 1000)\n",
    "  \n",
    "  for kernel_name, kernel, kernel_cdf in kernels:\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    idx, scores = plm(query_set[qid], index, docs_idx, tfidf_idx, doc_lengths, collection_size_nostop, id2tf, mu, kernel, kernel_cdf, sigma, rank_size)\n",
    "    \n",
    "    idx = [tfidf_idx[dum-1] for dum in idx] # index correction\n",
    "  \n",
    "    result_ids = [index.document(dum)[0] for dum in idx]\n",
    "     \n",
    "    tmp[qid] = tuple((scores[dum], result_ids[dum]) for dum in range(len(idx)))\n",
    "\n",
    "    code, res = run_trec_eval(tmp, 'PLM-'+kernel_name, metrics, groundtruth_qrel='./ap_88_89/qrel_test', qrun='./tmp_run', max_results=rank_size)\n",
    "    \n",
    "    if code == 0:\n",
    "      print('CODE ZERO',res)\n",
    "      \n",
    "    results_plm[i,:] = np.array(res)\n",
    "    \n",
    "    tmp.clear()\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "      print('- Query %d of %d: Kernel %s, elapsed time: %f' % (i+1,len(query_set), kernel_name, time.time() - start_time))\n",
    "\n",
    "with open('./plm_test_matrix', 'wb') as f:\n",
    "  pickle.dump(results_plm, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 39,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485534458443,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "49bc6e4cb281b53d",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "metrics = ['NDCG@10', 'P@5', 'MAP@10', 'R@1000']\n",
    "\n",
    "pkl_file = open('bm25_test_matrix', 'rb')\n",
    "mtrx = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "bm25_test = pd.DataFrame(mtrx, columns= ['BM25 ' + m for m in metrics])\n",
    "\n",
    "pkl_file = open('ad_test_matrix', 'rb')\n",
    "mtrx = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "ad_test = pd.DataFrame(mtrx, columns=['AD ' + m for m in metrics])\n",
    "\n",
    "\n",
    "pkl_file = open('dir_test_matrix', 'rb')\n",
    "mtrx = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "dir_test = pd.DataFrame(mtrx, columns=['DIR ' + m for m in metrics])\n",
    "\n",
    "\n",
    "pkl_file = open('jm_test_matrix', 'rb')\n",
    "mtrx = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "jm_test = pd.DataFrame(mtrx, columns=['JM ' + m for m in metrics])\n",
    "\n",
    "\n",
    "pkl_file = open('tfidf_test_matrix', 'rb')\n",
    "mtrx = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "tfidf_test = pd.DataFrame(mtrx, columns=['TFIDF ' + m for m in metrics])\n",
    "\n",
    "pkl_file = open('plm_test_matrix', 'rb')\n",
    "mtrx = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "plm_test = pd.DataFrame(mtrx, columns=['PLM ' + m for m in metrics])\n",
    "\n",
    "for i in range(len(metrics)):\n",
    "  col = metrics[i]\n",
    "  df2 = pd.concat([plm_test['PLM ' + col], jm_test['JM ' + col], ad_test['AD ' + col], bm25_test['BM25 ' + col], dir_test['DIR ' + col], tfidf_test['TFIDF ' + col]] , axis=1)\n",
    "  figure(figsize=(11, 3))\n",
    "  sns.barplot(data=df2)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 47,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485503322000,
     "user": {
      "color": "#1FA15D",
      "displayName": "Jose Daniel Gallego",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "17343136503924703230",
      "photoUrl": "//lh3.googleusercontent.com/-UeZcV3K1UA8/AAAAAAAAAAI/AAAAAAAABs0/qZwtdn6mzWc/s50-c-k-no/photo.jpg",
      "sessionId": "29ab36d7968363ad",
      "userId": "103200528046731184799"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "# Statistical test on dirichlet and plm. For different values, load different pickled files (saved during each test run)\n",
    "metrics = ['ndcg_cut.10', 'P.5', 'map_cut.10', 'recall.1000']\n",
    "\n",
    "pkl_file = open('plm_test_matrix', 'rb')\n",
    "table1 = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "pkl_file = open('dir_test_matrix', 'rb')\n",
    "table2 = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "# Computes the significance level applying the Šidák correction for having a final 5% alpha after a len(metrics) multiple comparisons test. https://en.wikipedia.org/wiki/%C5%A0id%C3%A1k_correction\n",
    "#corr_alpha = 1-(1-0.05)**(1/len(metrics))\n",
    "\n",
    "# Bonferroni correction\n",
    "corr_alpha = 0.05/len(metrics)\n",
    "\n",
    "\n",
    "for col in range(len(metrics)):\n",
    "  print('Measure: ' + metrics[col])\n",
    "  _ = sign_binomial_test(table2[:,col] - table1[:,col], 0, True, alternative='greater', sig = corr_alpha)\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Latent Semantic Models (LSMs) [25 points + 10 bonus points] ###\n",
    "\n",
    "In this task you will experiment with applying a distributional semantics methods ([word2vec](http://arxiv.org/abs/1411.2738)  **[5 points]**, [LSI](http://lsa3.colorado.edu/papers/JASIS.lsi.90.pdf) **[5 points]**, [LDA](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf) **[5 points]** and [doc2vec](https://cs.stanford.edu/~quocle/paragraph_vector.pdf) **[5 points]**) for retrieval.\n",
    "\n",
    "You do not need to implement word2vec, LSI, LDA and doc2vec on your own. Instead, you can use [gensim](http://radimrehurek.com/gensim/index.html) (pre-loaded on the VirtualBox). An example on how to integrate Pyndri with Gensim for word2vec can be found [here](https://github.com/cvangysel/pyndri/blob/master/examples/word2vec.py). For the remaining latent vector space models, you will need to implement connector classes (such as `IndriSentences`) by yourself.\n",
    "\n",
    "In order to use a latent semantic model for retrieval, you need to:\n",
    "   * build a representation of the query **q**,\n",
    "   * build a representation of the document **d**,\n",
    "   * calculate the similarity between **q** and **d** (e.g., cosine similarity, KL-divergence).\n",
    "     \n",
    "The exact implementation here depends on the latent semantic model you are using. For example, in the case of word2vec, you only have vectors for individual words and not for documents or phrases. Try one of the following methods for producing these representations:\n",
    "   * Average or sum the word vectors.\n",
    "   * Cluster words in the document using [k-means](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) and use the centroid of the most important cluster. Experiment with different values of K for k-means.\n",
    "   * Using the [bag-of-word-embeddings representation](https://ciir-publications.cs.umass.edu/pub/web/getpdf.php?id=1248). **[10 bonus points]**\n",
    "   \n",
    "Each of these LSMs come with various hyperparameters to tune. Make a choice on the parameters, and explicitly mention the reasons that led you to these decisions. You can use the validation set to optimize hyper parameters you see fit; motivate your decisions. In addition, mention clearly how the query/document representations were constructed for each LSM and explain your choices.\n",
    "\n",
    "In this experiment, you will first obtain an initial top-1000 ranking for each query using TF-IDF in **Task 1**, and then re-rank the documents using the LSMs. Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "Perform significance testing **[5 points]** (similar as in Task 1) in the class of semantic matching methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 17,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485528586674,
     "user": {
      "color": "#1FA15D",
      "displayName": "Dana Kianfar",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "04679598140879219033",
      "photoUrl": "//lh4.googleusercontent.com/-wuXN9UJDeVQ/AAAAAAAAAAI/AAAAAAAAL38/QdqQHlnDG58/s50-c-k-no/photo.jpg",
      "sessionId": "4ed2378f242e9e83",
      "userId": "112529955284751587675"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "#Connector classes\n",
    "import gensim, logging, bz2\n",
    "\n",
    "# For Word2vec. Provides a token stream from a document\n",
    "class IndriSentences(gensim.interfaces.CorpusABC):\n",
    "    \"\"\"Integrates an Index with Gensim's word2vec implementation.\"\"\"\n",
    "\n",
    "    def __init__(self, index, dictionary, max_documents=None):\n",
    "        assert isinstance(index, pyndri.Index)\n",
    "\n",
    "        self.index = index\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "        self.max_documents = max_documents\n",
    "\n",
    "    def _maximum_document(self):\n",
    "        if self.max_documents is None:\n",
    "            return self.index.maximum_document()\n",
    "        else:\n",
    "            return min(\n",
    "                self.max_documents + self.index.document_base(),\n",
    "                self.index.maximum_document())\n",
    "\n",
    "    def __iter__(self):\n",
    "        for int_doc_id in range(self.index.document_base(),\n",
    "                                self._maximum_document()):\n",
    "            ext_doc_id, tokens = self.index.document(int_doc_id)\n",
    "\n",
    "            yield tuple(\n",
    "                self.dictionary[token_id]\n",
    "                for token_id in tokens\n",
    "                if token_id > 0 and token_id in self.dictionary)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._maximum_document() - self.index.document_base()\n",
    "      \n",
    "# For Doc2Vec. Provides a token stream and document_id tuple\n",
    "class IndriDocs(gensim.interfaces.CorpusABC):\n",
    "    \"\"\"Integrates an Index with Gensim's word2vec implementation.\"\"\"\n",
    "\n",
    "    def __init__(self, index, dictionary, max_documents=None):\n",
    "        assert isinstance(index, pyndri.Index)\n",
    "\n",
    "        self.index = index\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "        self.max_documents = max_documents\n",
    "\n",
    "    def _maximum_document(self):\n",
    "        if self.max_documents is None:\n",
    "            return self.index.maximum_document()\n",
    "        else:\n",
    "            return min(\n",
    "                self.max_documents + self.index.document_base(),\n",
    "                self.index.maximum_document())\n",
    "\n",
    "    def __iter__(self):\n",
    "        for int_doc_id in range(self.index.document_base(),\n",
    "                                self._maximum_document()):\n",
    "            ext_doc_id, tokens = self.index.document(int_doc_id)\n",
    "\n",
    "            yield ext_doc_id, tuple(\n",
    "                self.dictionary[token_id]\n",
    "                for token_id in tokens\n",
    "                if token_id > 0 and token_id in self.dictionary)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._maximum_document() - self.index.document_base()      \n",
    "      \n",
    "# For LDA/LSI: provides a BOW dictionary constructed as {token_id:count} for each document  \n",
    "class IndriBow(gensim.interfaces.CorpusABC):\n",
    "    \"\"\"Integrates an Index with Gensim's word2vec implementation.\"\"\"\n",
    "\n",
    "    def __init__(self, index, docs_idx, max_documents=None):\n",
    "        assert isinstance(index, pyndri.Index)\n",
    "\n",
    "        self.index = index\n",
    "        self.docs_bow = docs_idx\n",
    "        self.max_documents = max_documents\n",
    "\n",
    "    def _maximum_document(self):\n",
    "        if self.max_documents is None:\n",
    "            return self.index.maximum_document()\n",
    "        else:\n",
    "            return min(\n",
    "                self.max_documents + self.index.document_base(),\n",
    "                self.index.maximum_document())\n",
    "\n",
    "    def __iter__(self):\n",
    "        for int_doc_id in self.docs_bow:\n",
    "            yield sorted(self.docs_bow[int_doc_id].items())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._maximum_document() - self.index.document_base()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  LDA/LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 18,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485528590300,
     "user": {
      "color": "#1FA15D",
      "displayName": "Dana Kianfar",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "04679598140879219033",
      "photoUrl": "//lh4.googleusercontent.com/-wuXN9UJDeVQ/AAAAAAAAAAI/AAAAAAAAL38/QdqQHlnDG58/s50-c-k-no/photo.jpg",
      "sessionId": "4ed2378f242e9e83",
      "userId": "112529955284751587675"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "# Utility classes\n",
    "\n",
    "# Gets document vectors from LDA/LSI\n",
    "# Inputs: \n",
    "# - model: LSA/LSI\n",
    "# - bow_dict: bag of words dictionary\n",
    "def get_vector_for_bows(bow_dict, model):\n",
    "  bow = [[k,v] for k, v in bow_dict.items()]\n",
    "  topics_scores = model[bow]\n",
    "  topics_idx = [topic_id for topic_id, score in topics_scores]\n",
    "  topics_scores = [score for topic_id, score in topics_scores]\n",
    "  \n",
    "  vec = np.zeros(model.num_topics)\n",
    "  vec[topics_idx] = topics_scores\n",
    "  \n",
    "  return vec\n",
    "\n",
    "# Waterfall\n",
    "def get_vector_for_tokenids(token_ids, model):\n",
    "  return get_vector_for_bows(collections.Counter(token_ids), model)\n",
    "\n",
    "# Tokenizes a string and returns vector representation\n",
    "def get_vector_for_query(query, model,token2id):\n",
    "  tokens = normalize(query, index)\n",
    "  token_ids = [token2id[token] for token in tokens if not token == '' and token in token2id]\n",
    "  return get_vector_for_tokenids(token_ids, model)\n",
    "\n",
    "\n",
    "# returns negative cosine similarity (to reduce sorting overhead in numpy by avoiding inversion)\n",
    "def get_query_doc_score(query, document, model, token2id):\n",
    "  query_vec = get_vector_for_query(query, model, token2id)\n",
    "  doc_vec = get_vector_for_bows(document, model)\n",
    "  return - 1 + scipy.spatial.distance.cosine(query_vec, doc_vec)\n",
    "\n",
    "# LDA/LSI scoring\n",
    "# Inputs: \n",
    "# - query: query string\n",
    "# - filtered_idx: list of document_ids\n",
    "# - model: LSA or LSI\n",
    "# Returns results ranked by cosine similarity scores\n",
    "# - Ranked list of document ids\n",
    "# - Corresponding positive cosine similarity scores\n",
    "def lda_lsi_scoring(query, index, model, filtered_idx, docs_idx, token2id, docs_lengths):\n",
    "  \n",
    "  scores = np.zeros(len(filtered_idx))\n",
    "  \n",
    "  for i, d_idx in enumerate(filtered_idx):\n",
    "    doc_bow = docs_idx[d_idx]\n",
    "    \n",
    "    if docs_lengths[d_idx] == 0:\n",
    "      scores[i] = Inf\n",
    "      continue\n",
    "    \n",
    "    scores[i] = get_query_doc_score(query, doc_bow, model, token2id)\n",
    "    \n",
    "  sorted_filtered_idx = np.argsort(scores)\n",
    "  orig_sorted_idx = [filtered_idx[x] for x in sorted_filtered_idx]\n",
    "  orig_sorted_scores = [-scores[x] for x in sorted_filtered_idx]\n",
    "  \n",
    "  return orig_sorted_idx, orig_sorted_scores\n",
    "\n",
    "# Calls TREC Eval\n",
    "def lda_lsi_query_vals (model, queries, qrel_fname, metrics, index, docs_idx, doc_lengths, token2id, out_fname):\n",
    "  \n",
    "  results = np.zeros((len(queries),len(metrics)))\n",
    "  tmp = {}\n",
    "  \n",
    "  for i, qid in enumerate(queries):\n",
    "    tmp.clear()\n",
    "    print('Query %d out of %d: %s' % (i+1, len(queries), queries[qid]))\n",
    "  \n",
    "    # TFIDF reranking\n",
    "    bm25_idx, bm25_scores = tfidf_scoring(queries[qid], index, docs_idx, doc_lengths, token2id, rank_size = 1000)\n",
    "    idx, scores = lda_lsi_scoring(queries[qid], index, model, bm25_idx, docs_idx, token2id, doc_lengths)\n",
    "    \n",
    "    result_ids = [index.document(i)[0] for i in idx]\n",
    "    tmp[qid] = tuple((scores[i], result_ids[i]) for i in range(len(idx)))\n",
    "\n",
    "    code, res = run_trec_eval(tmp, str(model), metrics, groundtruth_qrel=qrel_fname, qrun='./tmp_run', max_results=1000)\n",
    "    if code == 0:\n",
    "      print('CODE ZERO',res)\n",
    "    results[i,:] = np.array(res)\n",
    "  \n",
    "  with open(out_fname,'wb') as f:\n",
    "    pickle.dump(results,f)\n",
    "  \n",
    "  return results  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 79,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485462546184,
     "user": {
      "color": "#1FA15D",
      "displayName": "Dana Kianfar",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "04679598140879219033",
      "photoUrl": "//lh4.googleusercontent.com/-wuXN9UJDeVQ/AAAAAAAAAAI/AAAAAAAAL38/QdqQHlnDG58/s50-c-k-no/photo.jpg",
      "sessionId": "5affcb58e4a568be",
      "userId": "112529955284751587675"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "## Train LDA\n",
    "num_topics = 10\n",
    "eta = 1.0 / num_topics # uniform priors\n",
    "_alpha = 'symmetric'\n",
    "lda = gensim.models.ldamulticore.LdaMulticore(\n",
    "    corpus=IndriBow(index,docs_idx), num_topics=num_topics, \n",
    "    id2word=id2token, workers=3, chunksize=5000, passes=1, \n",
    "    batch=False, alpha=_alpha, eta=eta, eval_every=1000, iterations=5, minimum_probability=0.1)\n",
    "\n",
    "lda.save('./lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 66,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485347950584,
     "user": {
      "color": "#1FA15D",
      "displayName": "Dana Kianfar",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "04679598140879219033",
      "photoUrl": "//lh4.googleusercontent.com/-wuXN9UJDeVQ/AAAAAAAAAAI/AAAAAAAAL38/QdqQHlnDG58/s50-c-k-no/photo.jpg",
      "sessionId": "60254dd956d5cc42",
      "userId": "112529955284751587675"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "## Train LSI\n",
    "num_topics = 100\n",
    "lsi = gensim.models.lsimodel.LsiModel(corpus=IndriBow(index,docs_idx), id2word=id2token, num_topics=num_topics)\n",
    "lsi.save('./lsi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reported results on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 40,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485482978854,
     "user": {
      "color": "#1FA15D",
      "displayName": "Dana Kianfar",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "04679598140879219033",
      "photoUrl": "//lh4.googleusercontent.com/-wuXN9UJDeVQ/AAAAAAAAAAI/AAAAAAAAL38/QdqQHlnDG58/s50-c-k-no/photo.jpg",
      "sessionId": "28108fc55fbab36c",
      "userId": "112529955284751587675"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "## LSI test reuslts\n",
    "lsi = gensim.models.lsimodel.LsiModel.load('./lsi')\n",
    "metrics = ['ndcg_cut.10', 'P.5', 'map_cut.10', 'recall.1000']\n",
    "results = lda_lsi_query_vals(lsi, test_queries, './ap_88_89/qrel_test', metrics, index, docs_idx, doc_lengths, token2id, './lsi_test_results')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 41,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485482978895,
     "user": {
      "color": "#1FA15D",
      "displayName": "Dana Kianfar",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "04679598140879219033",
      "photoUrl": "//lh4.googleusercontent.com/-wuXN9UJDeVQ/AAAAAAAAAAI/AAAAAAAAL38/QdqQHlnDG58/s50-c-k-no/photo.jpg",
      "sessionId": "28108fc55fbab36c",
      "userId": "112529955284751587675"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "## LDA test reuslts\n",
    "lda = gensim.models.LdaModel.load('./lda')\n",
    "metrics = ['ndcg_cut.10', 'P.5', 'map_cut.10', 'recall.1000']\n",
    "results = lda_lsi_query_vals(lda, test_queries, './ap_88_89/qrel_test', metrics, index, docs_idx, doc_lengths, token2id, './lda_test_results')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Word2Vec / Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 19,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485528595247,
     "user": {
      "color": "#1FA15D",
      "displayName": "Dana Kianfar",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "04679598140879219033",
      "photoUrl": "//lh4.googleusercontent.com/-wuXN9UJDeVQ/AAAAAAAAAAI/AAAAAAAAL38/QdqQHlnDG58/s50-c-k-no/photo.jpg",
      "sessionId": "4ed2378f242e9e83",
      "userId": "112529955284751587675"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "#  Gets a vector embedding for a document\n",
    "# Inputs:\n",
    "# - model: Word2Vec, Doc2Vec\n",
    "# - doc_bow: bag-of-words dictionary of document {token_id: count}\n",
    "def get_docbow_embedding(doc_bow, model, doc_len, doc_idx=None ):\n",
    "  \n",
    "  if isinstance(model, gensim.models.Doc2Vec) and doc_idx is not None:\n",
    "    return model.docvecs[index.document(doc_idx)[0]]\n",
    "  \n",
    "  elif isinstance(model, gensim.models.Word2Vec) or  doc_idx is None: # Average Word2Vec vectors\n",
    "    res = np.zeros(model.vector_size)\n",
    "\n",
    "    for key, val in doc_bow.items():\n",
    "      if id2token[key] in model:\n",
    "        res += val * model[id2token[key]]\n",
    "\n",
    "    return res/doc_len\n",
    "\n",
    "# Word2Vec / Doc2vec scoring\n",
    "# Inputs: \n",
    "# - query: query string\n",
    "# - filtered_idx: list of document_ids\n",
    "# - model: Word2Vec or Doc2vec\n",
    "# Returns results ranked by cosine similarity scores\n",
    "# - Ranked list of document ids\n",
    "# - Corresponding positive cosine similarity scores\n",
    "def w2v_d2v_scoring(query, index, model, filtered_idx, docs_idx, doc_lengths):\n",
    "  \n",
    "  tokens = [token2id[t] for t in normalize(query, index) if not t == '' and t in token2id]\n",
    "  query_bow = collections.Counter(tokens)\n",
    "  query_vec = get_docbow_embedding(query_bow, model, len(tokens))\n",
    "  \n",
    "  scores = np.zeros(len(filtered_idx))   \n",
    "  \n",
    "  for i, d_idx in enumerate(filtered_idx):\n",
    "    \n",
    "    doc_bow = docs_idx[d_idx]    \n",
    "    \n",
    "    if doc_lengths[d_idx] == 0:\n",
    "      scores[i] = Inf\n",
    "      continue\n",
    "      \n",
    "    doc_vec = get_docbow_embedding(doc_bow, model, doc_lengths[d_idx], None)\n",
    "    scores[i] = -1 + scipy.spatial.distance.cosine(query_vec, doc_vec)  #inverted sign for not reversing list after argsort\n",
    "\n",
    "  \n",
    "  sorted_filtered_idx = np.argsort(scores)\n",
    "  orig_sorted_idx = [filtered_idx[x] for x in sorted_filtered_idx]\n",
    "  orig_sorted_scores = [-scores[x] for x in sorted_filtered_idx]\n",
    "  \n",
    "  return orig_sorted_idx, orig_sorted_scores\n",
    "\n",
    "# Calls trec eval\n",
    "def w2v_d2v_eval (model, queries, qrel_fname, metrics, index, docs_idx, doc_lengths, token2id, out_fname):\n",
    "  \n",
    "  results = np.zeros((len(queries),len(metrics)))\n",
    "  tmp = {}\n",
    "  \n",
    "  for i, qid in enumerate(queries):\n",
    "    tmp.clear()\n",
    "    print('Query %d out of %d: %s' % (i+1, len(queries), queries[qid]))\n",
    "  \n",
    "    #TFIDF reranking \n",
    "    bm25_idx, bm25_scores = tfidf_scoring(queries[qid], index, docs_idx, doc_lengths, token2id, rank_size = 1000)\n",
    "    idx, scores = w2v_d2v_scoring(queries[qid], index, model, bm25_idx, docs_idx, doc_lengths)\n",
    "    \n",
    "    result_ids = [index.document(i)[0] for i in idx]\n",
    "    tmp[qid] = tuple((scores[i], result_ids[i]) for i in range(len(idx)))\n",
    "\n",
    "    code, res = run_trec_eval(tmp, str(model), metrics, groundtruth_qrel=qrel_fname, qrun='./tmp_run', max_results=1000)\n",
    "    if code == 0:\n",
    "      print('CODE ZERO',res)\n",
    "    results[i,:] = np.array(res)\n",
    "  \n",
    "  with open(out_fname,'wb') as f:\n",
    "    pickle.dump(results,f)\n",
    "  \n",
    "  return results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "ename": "KeyboardInterrupt",
      "engine_info": {
       "engine_id": -1,
       "engine_uuid": "6079115f-388b-4358-8800-c28e44d98ec6",
       "method": "execute"
      },
      "evalue": "",
      "execution_count": 27,
      "payload": [],
      "status": "error",
      "traceback": [
       "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
       "\u001b[1;32m<ipython-input-27-b5c6539b9e56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Number of negative examples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0miter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Number of iterations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Number of workers.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m )\n",
       "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words)\u001b[0m\n\u001b[0;32m    467\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGeneratorType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You can't pass a generator as the sentences argument. Try an iterator.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    470\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, sentences, keep_raw_vocab, trim_rule, progress_per, update)\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m         \"\"\"\n\u001b[1;32m--> 533\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# initial survey\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# trim by min_count & precalculate downsampling\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinalize_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# build tables & arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[1;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[0mchecked_string_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msentence_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    546\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m<ipython-input-25-1a4bf6f1790c>\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m             yield tuple(\n\u001b[0;32m     29\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mtoken_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m                 if token_id > 0 and token_id in self.dictionary)\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m<ipython-input-25-1a4bf6f1790c>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mtoken_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                 if token_id > 0 and token_id in self.dictionary)\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/pyndri/__init__.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_term\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             tokenize(escape(string))]\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/pyndri/__init__.py\u001b[0m in \u001b[0;36mescape\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     })\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
      ],
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485204130960,
     "user": {
      "color": "#1FA15D",
      "displayName": "Dana Kianfar",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "04679598140879219033",
      "photoUrl": "//lh4.googleusercontent.com/-wuXN9UJDeVQ/AAAAAAAAAAI/AAAAAAAAL38/QdqQHlnDG58/s50-c-k-no/photo.jpg",
      "sessionId": "403efbbdf3c4e98d",
      "userId": "112529955284751587675"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "# Train word2vec\n",
    "print('Loading vocabulary.')\n",
    "sentences = IndriSentences(index, id2token)\n",
    "print('Initializing word2vec.')\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(sentences,\n",
    "    size=16,  # Embedding size\n",
    "    window=5,  # One-sided window size\n",
    "    sg=True,  # Skip-gram.\n",
    "    min_count=5,  # Minimum word frequency.\n",
    "    sample=1e-3,  # Sub-sample threshold.\n",
    "    hs=False,  # Hierarchical softmax.\n",
    "    negative=10,  # Number of negative examples.\n",
    "    iter=1,  # Number of iterations.\n",
    "    workers=3,  # Number of workers.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 54,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485207102549,
     "user": {
      "color": "#1FA15D",
      "displayName": "Dana Kianfar",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "04679598140879219033",
      "photoUrl": "//lh4.googleusercontent.com/-wuXN9UJDeVQ/AAAAAAAAAAI/AAAAAAAAL38/QdqQHlnDG58/s50-c-k-no/photo.jpg",
      "sessionId": "48ff6ff95edd1fcd",
      "userId": "112529955284751587675"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "#Train Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "py_docs = IndriDocs(index,id2token)\n",
    "\n",
    "doc_db = [TaggedDocument(words=text, tags=[docid]) for docid, text in py_docs]\n",
    "\n",
    "doc2vec_model = gensim.models.Doc2Vec(doc_db, size=16, window=5, min_count=1, workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Experiments with pre-trained word/document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 41,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485533289245,
     "user": {
      "color": "#1FA15D",
      "displayName": "Dana Kianfar",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "04679598140879219033",
      "photoUrl": "//lh4.googleusercontent.com/-wuXN9UJDeVQ/AAAAAAAAAAI/AAAAAAAAL38/QdqQHlnDG58/s50-c-k-no/photo.jpg",
      "sessionId": "4ed2378f242e9e83",
      "userId": "112529955284751587675"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "## Word2Vec test results\n",
    "pretrained_word2vec = gensim.models.Word2Vec.load('./word2vec.bin')\n",
    "metrics = ['ndcg_cut.10', 'P.5', 'map_cut.10', 'recall.1000']\n",
    "results = w2v_d2v_eval(pretrained_word2vec, test_queries, './ap_88_89/qrel_test', metrics, index, docs_idx, doc_lengths, token2id, './w2v_test_results')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 42,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485533971573,
     "user": {
      "color": "#1FA15D",
      "displayName": "Dana Kianfar",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "04679598140879219033",
      "photoUrl": "//lh4.googleusercontent.com/-wuXN9UJDeVQ/AAAAAAAAAAI/AAAAAAAAL38/QdqQHlnDG58/s50-c-k-no/photo.jpg",
      "sessionId": "4ed2378f242e9e83",
      "userId": "112529955284751587675"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "## Doc2Vec test reuslts\n",
    "pretrained_doc2vec = gensim.models.Doc2Vec.load('./doc2vec.bin')\n",
    "metrics = ['ndcg_cut.10', 'P.5', 'map_cut.10', 'recall.1000']\n",
    "results = w2v_d2v_eval(pretrained_doc2vec, test_queries, './ap_88_89/qrel_test', metrics, index, docs_idx, doc_lengths, token2id, './d2v_test_results')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots (LDA, LSI, W2V, D2V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 51,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485537361112,
     "user": {
      "color": "#1FA15D",
      "displayName": "Dana Kianfar",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "04679598140879219033",
      "photoUrl": "//lh4.googleusercontent.com/-wuXN9UJDeVQ/AAAAAAAAAAI/AAAAAAAAL38/QdqQHlnDG58/s50-c-k-no/photo.jpg",
      "sessionId": "4ed2378f242e9e83",
      "userId": "112529955284751587675"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "## Plotting\n",
    "\n",
    "metrics = ['NDCG@10', 'P@5', 'MAP@10', 'R@1000']\n",
    "\n",
    "pkl_file = open('lda_test_results', 'rb')\n",
    "mtrx = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "lda_test = pd.DataFrame(mtrx, columns= ['LDA ' + m for m in metrics])\n",
    "\n",
    "pkl_file = open('lsi_test_results', 'rb')\n",
    "mtrx = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "lsi_test = pd.DataFrame(mtrx, columns=['LSI ' + m for m in metrics])\n",
    "\n",
    "\n",
    "pkl_file = open('w2v_test_results', 'rb')\n",
    "mtrx = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "w2v_test = pd.DataFrame(mtrx, columns=['W2V ' + m for m in metrics])\n",
    "\n",
    "\n",
    "pkl_file = open('d2v_test_results', 'rb')\n",
    "mtrx = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "d2v_test = pd.DataFrame(mtrx, columns=['D2V ' + m for m in metrics])\n",
    "\n",
    "for i in range(len(metrics)):\n",
    "  col = metrics[i]\n",
    "  df2 = pd.concat([lda_test['LDA ' + col], lsi_test['LSI ' + col], w2v_test['W2V ' + col], d2v_test['D2V ' + col]] , axis=1)\n",
    "  figure(figsize=(8, 4))\n",
    "  sns.barplot(data=df2)\n",
    "  plt.savefig('./semantic_%s.png' % col)\n",
    "  plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Tests (LDA, LSI, W2V, D2V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 71,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485539698784,
     "user": {
      "color": "#1FA15D",
      "displayName": "Dana Kianfar",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "04679598140879219033",
      "photoUrl": "//lh4.googleusercontent.com/-wuXN9UJDeVQ/AAAAAAAAAAI/AAAAAAAAL38/QdqQHlnDG58/s50-c-k-no/photo.jpg",
      "sessionId": "4ed2378f242e9e83",
      "userId": "112529955284751587675"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "#%load_ext memory_profiler\n",
    "#%memit\n",
    "metrics = ['ndcg_cut.10', 'P.5', 'map_cut.10'] # , 'recall.1000'\n",
    "\n",
    "pkl_file = open('lda_test_results', 'rb')\n",
    "lda_data = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "pkl_file = open('lsi_test_results', 'rb')\n",
    "lsi_data = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "pkl_file = open('w2v_test_results', 'rb')\n",
    "w2v_data = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "pkl_file = open('d2v_test_results', 'rb')\n",
    "d2v_data = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "# Computes the significance level applying the Šidák correction for having a final 5% alpha after a len(metrics) multiple comparisons test. https://en.wikipedia.org/wiki/%C5%A0id%C3%A1k_correction\n",
    "corr_alpha = 1-(1-0.05)**(1/len(metrics))\n",
    "print(corr_alpha)\n",
    "corr_alpha = 0.05/len(metrics)\n",
    "print(corr_alpha)\n",
    "\n",
    "def compute_sb_test(table1, table2, corr_alpha):\n",
    "  print(table1.shape, table2.shape)\n",
    "  for col in range(len(metrics)):\n",
    "    print('Measure: ' + metrics[col])\n",
    "    _ = sign_binomial_test(table1[:,col] - table2[:,col], 0, True, alternative='greater', sig = corr_alpha)\n",
    "  print('')\n",
    "\n",
    "print('LDA > LSI')\n",
    "compute_sb_test(lda_data, lsi_data, corr_alpha)\n",
    "print('LDA > W2V')\n",
    "compute_sb_test(lda_data, w2v_data, corr_alpha)\n",
    "print('LDA > D2V')\n",
    "compute_sb_test(lda_data, d2v_data, corr_alpha)\n",
    "\n",
    "print('LSI > LDA')\n",
    "compute_sb_test(lsi_data, lda_data, corr_alpha)\n",
    "print('LSI > W2V')\n",
    "compute_sb_test(lsi_data, w2v_data, corr_alpha)\n",
    "print('LSI > D2V')\n",
    "compute_sb_test(lsi_data, d2v_data, corr_alpha)\n",
    "\n",
    "\n",
    "print('W2V > LDA')\n",
    "compute_sb_test(w2v_data, lda_data, corr_alpha)\n",
    "print('W2V > LSI')\n",
    "compute_sb_test(w2v_data, lsi_data, corr_alpha)\n",
    "print('W2V > D2V')\n",
    "compute_sb_test(w2v_data, d2v_data, corr_alpha)\n",
    "\n",
    "\n",
    "print('D2V > LDA')\n",
    "compute_sb_test(d2v_data, lda_data, corr_alpha)\n",
    "print('D2V > W2V')\n",
    "compute_sb_test(d2v_data, w2v_data, corr_alpha)\n",
    "print('D2V > LSI')\n",
    "compute_sb_test(d2v_data, lsi_data, corr_alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Learning to rank (LTR) [10 points] ###\n",
    "\n",
    "In this task you will get an introduction into learning to rank for information retrieval. You will experiment with a pointwise learning to rank method, logistic regression, implemented in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "**NOTE**: you can only perform this task if you have completely finished Task 1 and Task 2.\n",
    "\n",
    "In this experiment, you will use the retrieval methods you implemented in Task 1 and Task 2 as features for the learning to rank model. Train your LTR model using 10-fold cross validation on the test set. For every query, first create a document candidate set using the top-1000 documents using TF-IDF. Secondly, compute query-document values using the retrieval models above and use them as features. Note that the feature values of different retrieval methods are likely to be distributed differently.\n",
    "\n",
    "Your approach will definitely not be as good as the state-of-the-art since you are taking a pointwise approach, but we do not ask you to try pair- or listwise methods because they will be the main topic of the next assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 20,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485528634827,
     "user": {
      "color": "#1FA15D",
      "displayName": "Dana Kianfar",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "04679598140879219033",
      "photoUrl": "//lh4.googleusercontent.com/-wuXN9UJDeVQ/AAAAAAAAAAI/AAAAAAAAL38/QdqQHlnDG58/s50-c-k-no/photo.jpg",
      "sessionId": "4ed2378f242e9e83",
      "userId": "112529955284751587675"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "# Retrieves features (ranking scores) for a query and a single document\n",
    "# Returns a vector of size 10\n",
    "# Inputs:\n",
    "# - qid: query id\n",
    "# - filtered_doc_idx: id of document\n",
    "# - query_set: dictionary of queries\n",
    "def get_features_for_query(qid, filtered_doc_idx, query_set, index, mini_docs_idx, doc_lengths, collection_size_nostop, id2tf, token2id):\n",
    "  \n",
    "    # Contains the features of the current (d,q)\n",
    "    features = []\n",
    "    \n",
    "    # Adds query identifier for filtering later\n",
    "    features.append(int(qid))\n",
    "    \n",
    "    # Calculates score for \n",
    "    score =  tfidf_scoring(query_set[qid], index, mini_docs_idx, doc_lengths, token2id, rank_size = 1, sort_results = False)[0]\n",
    "    features.append(score)\n",
    "    \n",
    "    score = abs_discounting(query_set[qid], index, mini_docs_idx, doc_lengths, collection_size_nostop, id2tf, 0.8, rank_size = 1, sort_results = False)[0]\n",
    "    features.append(score)\n",
    "    \n",
    "    score = jelinek_mercer(query_set[qid], index, mini_docs_idx, doc_lengths, collection_size_nostop, id2tf, 0.4, rank_size = 1, sort_results = False)[0]\n",
    "    features.append(score)\n",
    "    \n",
    "    score = bm25(query_set[qid], index, mini_docs_idx, doc_lengths, token2id, avg_doc_length, k1=1.2, b=0.75, rank_size = 1, sort_results = False)[0]\n",
    "    features.append(score)\n",
    "    \n",
    "    score = dirichlet_smoothing(query_set[qid], index, mini_docs_idx, doc_lengths, collection_size_nostop, id2tf, 2000, rank_size = 1, sort_results = False)[0]\n",
    "    features.append(score)\n",
    "    \n",
    "    score = plm(query_set[qid], index, mini_docs_idx, [filtered_doc_idx], doc_lengths, collection_size_nostop, id2tf, 2000, triangle_kernel, triangle_cdf,\\\n",
    "                50, rank_size=1, sort_results = False)[0]\n",
    "    features.append(score)\n",
    "   \n",
    "    _, score = lda_lsi_scoring(query_set[qid], index, lda, [filtered_doc_idx], mini_docs_idx, token2id, doc_lengths)\n",
    "    features.append(score[0])\n",
    "   \n",
    "    _, score = lda_lsi_scoring(query_set[qid], index, lsi, [filtered_doc_idx], mini_docs_idx, token2id, doc_lengths)\n",
    "    features.append(score[0])\n",
    "   \n",
    "    _, score = w2v_d2v_scoring(query_set[qid], index, pretrained_word2vec, [filtered_doc_idx], mini_docs_idx, doc_lengths)\n",
    "    features.append(score[0])\n",
    "    \n",
    "    _, score = w2v_d2v_scoring(query_set[qid], index, pretrained_doc2vec, [filtered_doc_idx], mini_docs_idx, doc_lengths)\n",
    "    features.append(score[0])\n",
    "    \n",
    "    return features  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 21,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485528636592,
     "user": {
      "color": "#1FA15D",
      "displayName": "Dana Kianfar",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "04679598140879219033",
      "photoUrl": "//lh4.googleusercontent.com/-wuXN9UJDeVQ/AAAAAAAAAAI/AAAAAAAAL38/QdqQHlnDG58/s50-c-k-no/photo.jpg",
      "sessionId": "4ed2378f242e9e83",
      "userId": "112529955284751587675"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "# Creates features for each query-document pair in the test set\n",
    "def create_features(query_set, index, docs_idx, doc_lengths, collection_size_nostop, id2tf, token2id):\n",
    "  \n",
    "  # List of lists of features\n",
    "  res_feat = []\n",
    "  \n",
    "  # Lists of targets (1:R, 0:NR)\n",
    "  res_tgt = []\n",
    "  \n",
    "  i = 0\n",
    "\n",
    "  f = open('./ap_88_89/qrel_test', 'r')\n",
    "  for line in f:\n",
    "    \n",
    "    sp_line = line.split()\n",
    "    \n",
    "    qid = sp_line[0]\n",
    "    \n",
    "    _idx_tpl = index.document_ids([sp_line[2]])\n",
    "    \n",
    "    # Some documents dont have idx, so the previous tuple is empty\n",
    "    if len(_idx_tpl) == 0:\n",
    "      continue\n",
    "      \n",
    "    # Takes the document id from the tuple\n",
    "    idx = _idx_tpl[0][1]\n",
    "    \n",
    "    # Creates a mini docs_idx with just the document in the current line\n",
    "    mini_docs_idx = {idx: docs_idx[idx]}\n",
    "    \n",
    "    # After constructing the features list append it to res_feat\n",
    "    res_feat.append(get_features_for_query(qid, idx, query_set, index, mini_docs_idx, doc_lengths, collection_size_nostop, id2tf, token2id))\n",
    "    \n",
    "    rel_label = float(sp_line[-1])\n",
    "    res_tgt.append(rel_label)\n",
    "    \n",
    "    if (i+1) % 1000 == 0:\n",
    "      print('Q ' + str(i))\n",
    "      \n",
    "    i += 1\n",
    "    \n",
    "  res_feat = np.array(res_feat)\n",
    "  res_tgt = np.array(res_tgt)\n",
    "  \n",
    "  return res_feat, res_tgt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 31,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485527166979,
     "user": {
      "color": "#1FA15D",
      "displayName": "Dana Kianfar",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "04679598140879219033",
      "photoUrl": "//lh4.googleusercontent.com/-wuXN9UJDeVQ/AAAAAAAAAAI/AAAAAAAAL38/QdqQHlnDG58/s50-c-k-no/photo.jpg",
      "sessionId": "762af6763480169b",
      "userId": "112529955284751587675"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "# Generate and store training data for LTR\n",
    "res_feat, res_tgt = create_features(test_queries, index, docs_idx, doc_lengths, collection_size_nostop, id2tf, token2id)\n",
    "with open('./ltr_training','wb') as f:\n",
    "  pickle.dump(res_feat, f)\n",
    "  pickle.dump(res_tgt, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 32,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485531148083,
     "user": {
      "color": "#1FA15D",
      "displayName": "Dana Kianfar",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "04679598140879219033",
      "photoUrl": "//lh4.googleusercontent.com/-wuXN9UJDeVQ/AAAAAAAAAAI/AAAAAAAAL38/QdqQHlnDG58/s50-c-k-no/photo.jpg",
      "sessionId": "4ed2378f242e9e83",
      "userId": "112529955284751587675"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import sklearn\n",
    "\n",
    "# Trains a binary classifier (logistic, linear or MLP) with k-fold cross validation\n",
    "# Inputs:\n",
    "# - X_raw: unscaled dataframe of data, first columns is query id\n",
    "# - t: targets (binary)\n",
    "# - cv_k: number of folds (k) in cross validation \n",
    "# - model: \n",
    "#    - log: logistic regression\n",
    "#    - lin: linear regression\n",
    "#    - nn: multi layer perceptron\n",
    "# - scale: flag for normalizing data\n",
    "# Returns\n",
    "# - List of models, their train/test queries, and their scalers\n",
    "def train_ltr_models_cv(X_raw, t, cv_k = 10, model = 'log', scale=False):\n",
    "  \n",
    "  # List of unique query ids\n",
    "  query_ids = X_raw['query'].unique()\n",
    "  data_cols = list(X_raw.drop(['query'], axis=1).columns) \n",
    "  ltr_models = [] \n",
    "  \n",
    "  # Do PCA and save transformations\n",
    "  X = pd.DataFrame.copy(X_raw)\n",
    "  scaler = None\n",
    "  if scale:\n",
    "    scaler = preprocessing.MinMaxScaler().fit(X_raw.drop(['query'], axis=1)) # exclude query ids columns in dataframe\n",
    "    X[data_cols] = scaler.transform(X_raw[data_cols])\n",
    "  \n",
    "  \n",
    "  # Do Kfold split\n",
    "  kf = KFold(n_splits=cv_k)\n",
    "  i = 0\n",
    "  for train, test in kf.split(query_ids): # perform split on query ids\n",
    "    \n",
    "    q_tr = [query_ids[x] for x in train] # training queries\n",
    "    q_te = [query_ids[x] for x in test]  # testing queries, used for reporting test accuracy and storing for bookkeeping\n",
    "    \n",
    "    print('Training model %d out of %d'% (i+1, cv_k))\n",
    "    i+=1\n",
    "    \n",
    "    # Slice up data for k-fold train/test split\n",
    "    tr_idx = X['query'].isin(q_tr) # filter by queries\n",
    "    X_train = X[tr_idx].drop(['query'], axis=1)\n",
    "    t_train = t[tr_idx]\n",
    "    \n",
    "    te_idx = X['query'].isin(q_te) # filter by queries\n",
    "    X_test = X[te_idx].drop(['query'], axis=1)\n",
    "    t_test = t[te_idx]\n",
    "    \n",
    "    \n",
    "    # Initialize model\n",
    "    if model == 'log':\n",
    "      ltr = LogisticRegression(tol=0.00001, fit_intercept=True, solver='sag',max_iter=5000, n_jobs=3, random_state=1)\n",
    "    elif model == 'lin':\n",
    "      ltr = sklearn.linear_model.LinearRegression()\n",
    "    else:\n",
    "      ltr = MLPClassifier(activation = 'logistic', solver='adam', alpha=1e-5, max_iter=500,  tol=1e-6, nesterovs_momentum = True\n",
    "                          , early_stopping=True, verbose=True, random_state=1, hidden_layer_sizes=(30))\n",
    "    \n",
    "    # Fit training data\n",
    "    ltr.fit(X_train.as_matrix(), t_train.as_matrix().ravel())\n",
    "    \n",
    "    # Repotr test accuracy\n",
    "    print('Model %d '%(i),ltr.score(X_test, t_test))\n",
    "\n",
    "    # Save\n",
    "    ltr_models.append({'model': ltr, 'tr_query': q_tr, 'te_query':q_te})\n",
    "    \n",
    "  return ltr_models, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 33,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485531576777,
     "user": {
      "color": "#1FA15D",
      "displayName": "Dana Kianfar",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "04679598140879219033",
      "photoUrl": "//lh4.googleusercontent.com/-wuXN9UJDeVQ/AAAAAAAAAAI/AAAAAAAAL38/QdqQHlnDG58/s50-c-k-no/photo.jpg",
      "sessionId": "4ed2378f242e9e83",
      "userId": "112529955284751587675"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "# Train models\n",
    "\n",
    "# Load up training data\n",
    "with open('./ltr_training','rb') as f:\n",
    "  res_feat = pickle.load( f)\n",
    "  res_tgt = pickle.load( f)\n",
    "\n",
    "# Store in dataframes    \n",
    "df_X = pd.DataFrame(res_feat, columns =['query','tfidf','ad','jm','bm25','dirichlet','plm','w2v', 'd2v'])\n",
    "df_t = pd.DataFrame(res_tgt,columns=['target'])\n",
    "print(df_X.shape, df_t.shape)\n",
    "del res_feat\n",
    "del res_tgt\n",
    "\n",
    "\n",
    "# Clean data by replacing infs (due to design)\n",
    "df_X['tfidf'].replace(-inf, -200, inplace=True)\n",
    "df_X['plm'].replace(-inf, -100, inplace=True)\n",
    "\n",
    "ltr_models, scaler = train_ltr_models_cv(df_X, df_t, cv_k = 10, model = 'log', scale=False)\n",
    "\n",
    "with open('kfold_cv_results_log','wb') as f:\n",
    "  pickle.dump(ltr_models,f)\n",
    "  pickle.dump(scaler,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 24,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485528810767,
     "user": {
      "color": "#1FA15D",
      "displayName": "Dana Kianfar",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "04679598140879219033",
      "photoUrl": "//lh4.googleusercontent.com/-wuXN9UJDeVQ/AAAAAAAAAAI/AAAAAAAAL38/QdqQHlnDG58/s50-c-k-no/photo.jpg",
      "sessionId": "4ed2378f242e9e83",
      "userId": "112529955284751587675"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "# Run Trec Eval\n",
    "def run_ltr_evals(ltr_models, scaler, queries, qrel_fname, metrics, index, docs_idx, doc_lengths, collection_size_nostop, id2tf, token2id, out_fname, rank_size):\n",
    "  \n",
    "  tmp = {}\n",
    "  \n",
    "  for k, model_settings in enumerate(ltr_models):\n",
    "    \n",
    "    print('Evaluating LTR model %d' % (k+1))\n",
    "    \n",
    "    model = model_settings['model']\n",
    "    test_queries_cv = model_settings['te_query']\n",
    "    test_queries_cv = [str(int(a)) for a in test_queries_cv]\n",
    "    #qid_ls = [ls_test_query_ids[int(_idx)] for _idx in test_queries_cv]\n",
    "    #print(qid_ls)\n",
    "    \n",
    "    for o,qid in enumerate(test_queries_cv):\n",
    "      \n",
    "      print('Query %d out of %d'% (o+1,len(test_queries_cv)))\n",
    "      \n",
    "    # TFIDF re-ranking\n",
    "      tfidf_idx, tfidf_scores = tfidf_scoring(queries[qid], index, docs_idx, doc_lengths, token2id, rank_size)\n",
    "      \n",
    "      features = []\n",
    "      \n",
    "        # features for each document\n",
    "      for _dox in tfidf_idx:\n",
    "        mini_docs_idx = {_dox: docs_idx[_dox]}\n",
    "        f = get_features_for_query(qid, _dox, queries, index, mini_docs_idx, doc_lengths, collection_size_nostop, id2tf, token2id)        \n",
    "        features.append(f)\n",
    "      \n",
    "      np_arr = numpy.array(features)\n",
    "      np_arr[:,1][np_arr[:,1] <= -float('Inf')] = -200\n",
    "      np_arr[:,6][np_arr[:,6] <= -float('Inf')] = -100\n",
    "      \n",
    "        #scale if necessary\n",
    "      if scaler:\n",
    "        np_arr[:,1:] = scaler.transform(np_arr[:,1:])\n",
    "      \n",
    "        # predict\n",
    "      preds = model.predict_proba(np_arr[:,1:])\n",
    "      \n",
    "        # sort predictions\n",
    "      sort_idx = np.argsort(preds[:,0])[::-1]\n",
    "      \n",
    "        # store sorted results for trec eval\n",
    "      tmp[qid] = tuple((preds[i,0], index.document(tfidf_idx[i])[0]) for i in sort_idx)\n",
    "      #print(qid, np_arr.shape)\n",
    "      \n",
    "      del np_arr\n",
    "      del features\n",
    "    \n",
    "    # run trev eval\n",
    "  code, res = run_trec_eval(tmp, 'LTR', metrics, groundtruth_qrel=qrel_fname, qrun='./tmp_run', max_results=rank_size)\n",
    "  print(res)\n",
    "  \n",
    "  if code == 0:\n",
    "      print('CODE ZERO',res)\n",
    "      \n",
    "  with open(out_fname,'wb') as f:\n",
    "    pickle.dump(tmp,f)\n",
    "    pickle.dump(res,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "executionInfo": {
     "content": {
      "execution_count": 35,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1485532264833,
     "user": {
      "color": "#1FA15D",
      "displayName": "Dana Kianfar",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "04679598140879219033",
      "photoUrl": "//lh4.googleusercontent.com/-wuXN9UJDeVQ/AAAAAAAAAAI/AAAAAAAAL38/QdqQHlnDG58/s50-c-k-no/photo.jpg",
      "sessionId": "4ed2378f242e9e83",
      "userId": "112529955284751587675"
     },
     "user_tz": -60
    }
   },
   "outputs": [],
   "source": [
    "# LTR performance on test data\n",
    "with open('kfold_cv_results_log_nolsilda','rb') as f:\n",
    "  ltr_models = pickle.load(f)\n",
    "  scaler = pickle.load(f)\n",
    "\n",
    "# we only consider re-rank top 100 from tfidf for performance considerations\n",
    "metrics = ['ndcg_cut.10', 'P.5', 'map_cut.10', 'recall.100']  \n",
    "\n",
    "# load models into memory\n",
    "lda = gensim.models.LdaMulticore.load('./lda')\n",
    "lsi = gensim.models.LsiModel.load('./lsi')\n",
    "pretrained_word2vec = gensim.models.Word2Vec.load('./word2vec.bin')\n",
    "pretrained_doc2vec = gensim.models.Doc2Vec.load('./doc2vec.bin')\n",
    "\n",
    "\n",
    "run_ltr_evals(ltr_models, scaler, queries, './ap_88_89/qrel_test', metrics, index, docs_idx, \\\n",
    "              doc_lengths, collection_size_nostop, id2tf, token2id, './ltr_treceval_results_log_nolsilda', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Write a report [20 points; instant FAIL if not provided] ###\n",
    "\n",
    "The report should be a PDF file created using the [sigconf ACM template](https://www.acm.org/publications/proceedings-template) and will determine a significant part of your grade.\n",
    "\n",
    "   * It should explain what you have implemented, motivate your experiments and detail what you expect to learn from them. **[10 points]**\n",
    "   * Lastly, provide a convincing analysis of your results and conclude the report accordingly. **[10 points]**\n",
    "      * Do all methods perform similarly on all queries? Why?\n",
    "      * Is there a single retrieval model that outperforms all other retrieval models (i.e., silver bullet)?\n",
    "      * ...\n",
    "\n",
    "**Hand in the report and your self-contained implementation source files.** Do not send us the VirtualBox, but only the files that matter, organized in a well-documented zip/tgz file with clear instructions on how to reproduce your results. That is, we want to be able to regenerate all your results with minimal effort. You can assume that the index and ground-truth information is present in the same file system structure as on the VirtualBox.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colabVersion": "0.1",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
